\section{Background}\label{sec:background}

\subsection{Linear logic}\label{sec:linear-logic}

Here we review the sequent calculus presentation of \ac{ILL}.
The reader who is familiar with \acl{ILL} may easily (and perhaps should) skip this \lcnamecref{sec:linear-logic}.

\vskip 3.25ex plus 1ex minus .2ex\noindent
Like a good witness, traditional intuitionistic logic is concerned only with facts.
Facts are \vocab{persistent};
% Traditional intuitionistic logic is concerned with the \emph{truth} of propositions.
% Truths are \wc{forever};
having proved a proposition true, there is no \wc{harm}[\st{cost}] in reusing or even forgetting the proof.
Traditional intuitionistic logic therefore validates several structural principles: \vocab{contraction}, that hypotheses may be duplicated; \vocab{weakening}, that irrelevant hypotheses may be ignored; and \vocab{exchange}, that the order of hypotheses is unimportant.
% Such hypotheses are said to be \vocab{persistent}.

Linear logic~\autocite{Girard:TCS87}, on the other hand, treats proofs as precious\fxnote{, nonrenewable} resources.
Accordingly, linear logic cannot validate the contraction and weakening principles: resources should be neither freely duplicated (as\fxnote{\ \st{they would be}} with contraction) nor freely discarded (as\fxnote{\ \st{they would be}} with weakening).
The order of resources remains unimportant, however, so linear logic does validate the exchange principle.
Such resources are said to be \vocab{ephemeral}.

We will now review a sequent calculus for intuitionistic linear logic from the judgmental reconstruction by~\textcite{Chang+:CMU03}; our presentation is also heavily indebted to lectures by \textcite{Pfenning:816}.

The main judgment is a linear hypothetical judgment, or \vocab{sequent}, which states that by using resources~$A_1, \dotsc, A_n$ we can obtain resource~$A$:%
\begin{equation*}
  \underbrace{A_1, \dotsc, A_n}_{\textstyle \lctx} \seq A \:.
\end{equation*}
Because resources may be permuted but neither duplicated nor discarded, the context of resources~$A_1, \dotsc, A_n$ is treated as a multiset.
Following tradition, we use the metavariable~$\lctx$ to stand for an arbitrary context of resources.


\ExplSyntaxOn
\NewDocumentCommand{\lmatch}{O{} m}{\lfill[#1]{#2}}
\NewDocumentCommand{\lfill}{O{} m}{
  \lctx#1\tl_if_empty:nF{#2}{, #2}
}
\ExplSyntaxOff
%

\subsubsection{Judgmental rules}\label{sec:judgmental-rules}

What are the rules for obtaining and using resources?
One way to obtain a resource $A$ is to use it directly.
Dually, if we can use some of our resources to obtain an $A$, then we are justified in later using that $A$\fxnote{\ \st{along with the remaining resources}}.
These judgmental principles are expressed by the identity and cut rules\footnote{The identity and cut rules can in fact be shown to be admissible~\autocites{Chang+:CMU03}{Pfenning:816Identity}, but we postpone discussion of this to \cref{sec:linear-lp}.}, respectively:
\begin{mathpar}
  \infer[\lab{id$_A$}]{A \seq A}{
    }
  \and
  \infer[\lab{cut$_A$}]{\lmatch[']{\lctx} \seq C}{
    \lctx \seq A &
    \lfill[']{A} \seq C} \:.
\end{mathpar}
Notice that the identity rule uses only an $A$; allowing any other resources on the sequent's left would cause them to be incorrectly disposed here.

\subsubsection{Propositional connectives}\label{sec:prop-conn}

In addition to the judgmental rules of identity and cut, the sequent calculus contains right and left rules that give meaning to each propositional connective.
% A proof of $A \lolli B$ is a plan for using resource $A$ to achieve goal $B$.
For instance, the \wc{linear implication} $A \lolli B$ (pronounced \enquote{$A$ linearly implies $B$} or \enquote{$A$ lolli $B$}) internalizes the linear hypothetical judgment as a proposition: a proof of $A \lolli B$ is a plan for using resource $A$ to obtain resource $B$.
% To make our plan, we presume to have resource $A$ and show how to acheive $B$.
To obtain such a plan, we can presume to have an $A$ available for use and show how to obtain a $B$:
\begin{equation*}
  \infer[\rlab{{\lolli}}]{\lctx \seq A \lolli B}{
    \lctx, A \seq B}
  \:.
\end{equation*}
% Conversely, we can use our plan to produce $B$ if we can obtain an $A$.
Conversely, if we can obtain an $A$, then we can carry out, \ie\ use, our plan to make a $B$ available for use:
% To use our plan, we must first obtain $A$
\begin{equation*}
  \infer[\llab{{\lolli}}]{\lmatch[']{\lctx'_A, A \lolli B} \seq C}{
    \lctx'_A \seq A &
    \lfill[']{B} \seq C}
  \:.
\end{equation*}
Thus, the right and left rules explain how to obtain and use, respectively, the resource $A \lolli B$.

The resource $A \tensor B$ (pronounced \enquote{$A$ tensor $B$}) is a pair of the resources $A$ and $B$.
Once again, the right and left rules give meaning to $A \tensor B$:
\begin{mathpar}
  \infer[\rlab{{\tensor}}]{\lctx_1, \lctx_2 \seq A \tensor B}{
    \lctx_1 \seq A &
    \lctx_2 \seq B}
  \and
  \infer[\llab{{\tensor}}]{\lmatch[']{A \tensor B} \seq C}{
    \lfill[']{A, B} \seq C}
\end{mathpar}
To obtain an $A \tensor B$, we must use some of our resources to obtain an $A$ and the rest to obtain a $B$.
To use an $A \tensor B$, we must use \emph{both} an $A$ and a $B$: using only one would cause the resources that went into obtaining the other to be incorrectly disposed.
%These \wc{properties} are expressed in the right and left rules, respectively:
Thus, $A \tensor B$ is a multiplicative conjunction in which both $A$ and $B$ must be used.

A different, additive form of conjunction is given by $A \with B$ (pronounced~\enquote{$A$ with~$B$}):
\begin{mathpar}
  \infer[\rlab{{\with}}]{\lctx \seq A \with B}{
    \lctx \seq A &
    \lctx \seq B}
  \and
  \infer[{\llab{{\with}}[1]}]{\lmatch[']{A \with B} \seq C}{
    \lfill[']{A} \seq C}
  \and
  \infer[{\llab{{\with}}[2]}]{\lmatch[']{A \with B} \seq C}{
    \lfill[']{B} \seq C}
\end{mathpar}
To obtain an $A \with B$, we must obtain an $A$ and a $B$ using \emph{all} of our resources, $\lctx$, in each case.
To use an $A \with B$, we must then choose exactly one of $A$ and $B$ to use; using both would be an underhanded duplication of the resources $\lctx$.

The rules for the additive disjunction $A \llor B$ are dual to those for $A \with B$:
\begin{mathpar}
  \infer[{\rlab{{\llor}}[1]}]{\lctx \seq A \llor B}{
    \lctx \seq A}
  \and
  \infer[{\rlab{{\llor}}[2]}]{\lctx \seq A \llor B}{
    \lctx \seq B}
  \and
  \infer[\llab{{\llor}}]{\lmatch[']{A \llor B} \seq C}{
    \lfill[']{A} \seq C &
    \lfill[']{B} \seq C}
\end{mathpar}
A resource $A \llor B$ is thus either an $A$ or a $B$.
When using a $A \llor B$, we must therefore be prepared to use whichever one it may be.
% This time, instead of choosing one of $A$ and $B$ when using $A \llor B$, the environment has made that choice for us; we must be prepared for either case.

\subsubsection{First-order quantifiers}\label{sec:first-order-quant}

\NewDocumentCommand{\univq}{u: u.}{\forall #1{:}#2.\,}
\ExplSyntaxOn
\NewDocumentCommand{\existq}{>{\SplitArgument{1}{:}}u.}{% \existq_:n {#1}}
% \NewDocumentCommand{\existq_:n}{>{\SplitArgument{1}{:}}m}{
  \exists \use_i:nn #1
  \exp_args:No \IfValueT { \use_ii:nn #1 }
    { \mathord{\colon} \use_ii:nn #1 }
  .\,
}
\ExplSyntaxOff
\NewDocumentCommand{\tctx}{}{\Psi}
\NewDocumentCommand{\subst}{m m}{[#1]#2}
% We can also include first-order universal and existential quantifiers, $\univq x:\tau.A$ and $\existq x:\tau.A$.
Linear logic also has first-order universal and existential quantifiers, $\univq x:\tau.A$ and $\existq x:\tau.A$.
% The sequent is extended with a context, $\tctx$, of parameters $x_i{:}\tau_i$ so that it becomes $\tctx ; \lctx \seq A$.
The sequent is extended with a context of parameter typings, $x_i{:}\tau_i$:
\begin{equation*}
  \underbrace{x_1{:}\tau_1, \dotsc, x_n{:}\tau_n}_{\displaystyle \tctx} \;;\, \lctx \seq A \:.
\end{equation*}
To match the new form of sequent, all previously presented rules are uniformly modified to include a parameter context $\tctx$ in their premises and conclusion.

Only the rules for the quantifiers directly affect the parameter context.
They are the same as for traditional intuitionistic logic except that the quantifiers are ephemeral:
% The inference rules for quantifiers are then the same as for intuitionistic logic, except that the quantifiers are \wc{ephemeral}[\st{consumed upon use}]:
\begin{mathpar}
  \infer[\rlab{\forall}]{\tctx ; \lctx \seq \univq x:\tau. A_x}{
    \tctx, x{:}\tau ; \lctx \seq A_x}
  \and
  \infer[\llab{\forall}]{\tctx ; \lmatch[']{\univq x:\tau. A_x} \seq C}{
    \tctx \seq M : \tau &
    \tctx ; \lfill[']{\subst{M/x}{A_x}} \seq C}
  \\
  \infer[\rlab{\exists}]{\tctx ; \lctx \seq \existq x:\tau. A_x}{
    \tctx \seq M : \tau &
    \tctx ; \lctx \seq \subst{M/x}{A_x}}
  \and
  \infer[\llab{\exists}]{\tctx ; \lmatch[']{\existq x:\tau. A_x} \seq C}{
    \tctx, x{:}\tau ; \lfill[']{A_x} \seq C}
\end{mathpar}

\subsubsection{Persistence}\label{sec:persistence}

Even in resource-centric linear logic, we sometimes want to express persistent facts.
% Finally, even in linear logic we sometimes want to allow a controlled degree of persistence.
% % certain kinds of hypotheses to be duplicated or ignored in a controlled way.
To do so, we introduce a new judgment, $A \pers$, for proofs that may not depend on ephemeral resources.
% These hypotheses are called \vocab{valid} because they may not depend on linear resources.
% To do so, we introduce a new judgment, $B \valid$, for these persistent hypotheses.
The sequent is extended with a context of $\pers$ antecedents:
% that admits the contraction and weakening principles:
\begin{equation*}
  % \underbrace{x_1{:}\tau_1, \dotsc, x_\ell{:}\tau_\ell}_{\displaystyle \tctx} \,; \underbrace{B_1, \dotsc, B_m}_{\displaystyle \uctx} \,; \underbrace{A_1, \dotsc, A_n}_{\displaystyle \lctx} \seq A \:.
  \tctx \,;\, \underbrace{A_1, \dotsc, A_n}_{\displaystyle \uctx} \;;\, \lctx \seq A \:.
\end{equation*}
Because they may not depend on ephemeral resources, there is no harm in duplicating or discarding $\pers$ antecedents.
The persistent context~$\uctx$ therefore admits contraction and weakening (as well as exchange).

Consequently, we can
% Because persistent facts may be reused indefinitely, we may
copy a fact and use it as an ephemeral resource.
On the other hand, if we can obtain an~$A$ without relying on any ephemeral resources, then we are justified in having~$A$ as a persistent fact.
These two judgmental principles are expressed by the $\lab{copy}$ and $\lab{cut$^{\bang}_A$}$ rules:
\begin{mathpar}
  \infer[\lab{copy}]{\tctx ; \uctx, A ; \lctx' \seq C}{
    \tctx ; \uctx, A ; \lctx', A \seq C}
  \and
  \infer[\lab{cut$^{\bang}_A$}]{\tctx ; \uctx ; \lctx' \seq C}{
    \tctx ; \uctx ; \lctxe \seq A &
    \tctx ; \uctx, A ; \lctx' \seq C}
  \:.
\end{mathpar}

Finally, 
% Therefore, 
to match the new form of sequent,
% all previously presented rules are modified to uniformly include a context $\uctx$ in their premises and conclusion.
the other rules of linear logic are uniformly modified to include a context $\uctx$ in their premises and conclusion.
% There is no harm in duplicating or discarding persistent facts because they may not depend on ephemeral resources, %:
% % we would not be violating the linearity of resources.
% so the context $\uctx$ admits contraction and weakening (as well as exchange).
% % Therefore, the context $\uctx$ of persistent facts is treated as a set that also admits weakening.%
% % \footnote{Technically speaking, $\uctx$ is actually a multiset, with contraction being admissible as well.}
% Therefore, to match the new form of sequent, all previously presented rules are once again modified to include a \wc{validity}[\st{persistent}] context $\uctx$ in their premises and conclusion.
% % Once again, to match the new form of sequent, all previously presented rules are uniformly modified to include a \wc{validity}[persistent] context $\uctx$ in the premises and conclusion.
% For instance, the left rule for $A \lolli B$ is now
% \begin{equation*}
%   \infer[\llab{{\lolli}}]{\tctx ; \uctx ; \lmatch[']{\lctx'_A, A \lolli B} \seq C}{
%     \tctx ; \uctx ; \lctx'_A \seq A &
%     \tctx ; \uctx ; \lfill[']{B} \seq C}
%   \:.
% \end{equation*}


\subsubsection{Summary}\label{sec:summary-linear-logic}

For convenient reference, the rules described above are summarized in their complete form in \cref{fig:seq-jill}.%
\fxerror{\ [Decide whether to present $\bang A$ and $\monad{A}$ here.]}

% In this section, we briefly review a judgmental intuitionistic formulation of \citeauthor{Girard:TCS87}'s linear logic~\autocite*{Girard:TCS87}, a substructural logic of resources that is especially suited to modeling stateful systems.



\begin{figure}
  \begin{mathpar}
    \infer[\lab{id$_A$}]{\tctx ; \uctx ; A \seq A}{
      }
    \and
    \infer[\lab{cut$_A$}]{\tctx ; \uctx ; \lmatch[']{\lctx} \seq C}{
      \tctx ; \uctx ; \lctx \seq A &
      \tctx ; \uctx ; \lfill[']{A} \seq C}
    \\
    \infer[\rlab{{\lolli}}]{\tctx ; \uctx ; \lctx \seq A \lolli B}{
      \tctx ; \uctx ; \lctx, A \seq B}
    \and
    \infer[\llab{{\lolli}}]{\tctx ; \uctx ; \lmatch[']{\lctx'_A, A \lolli B} \seq C}{
      \tctx ; \uctx ; \lctx'_A \seq A &
      \tctx ; \uctx ; \lfill[']{B} \seq C}
    \\
    \infer[\rlab{{\tensor}}]{\tctx ; \uctx ; \lctx_1, \lctx_2 \seq A \tensor B}{
      \tctx ; \uctx ; \lctx_1 \seq A &
      \tctx ; \uctx ; \lctx_2 \seq B}
    \and
    \infer[\llab{{\tensor}}]{\tctx ; \uctx ; \lmatch[']{A \tensor B} \seq C}{
      \tctx ; \uctx ; \lfill[']{A, B} \seq C}
    \\
    \infer[\rlab{\one}]{\tctx ; \uctx ; \lctxe \seq \one}{
      }
    \and
    \infer[\llab{\one}]{\tctx ; \uctx ; \lmatch[']{\one} \seq C}{
      \tctx ; \uctx ; \lfill[']{} \seq C}
    \\
    \infer[\rlab{{\with}}]{\tctx ; \uctx ; \lctx \seq A \with B}{
      \tctx ; \uctx ; \lctx \seq A &
      \tctx ; \uctx ; \lctx \seq B}
    \\
    \infer[{\llab{{\with}}[1]}]{\tctx ; \uctx ; \lmatch[']{A \with B} \seq C}{
      \tctx ; \uctx ; \lfill[']{A} \seq C}
    \and
    \infer[{\llab{{\with}}[2]}]{\tctx ; \uctx ; \lmatch[']{A \with B} \seq C}{
      \tctx ; \uctx ; \lfill[']{B} \seq C}
    \\
    \infer[{\rlab{{\llor}}[1]}]{\tctx ; \uctx ; \lctx \seq A \llor B}{
      \tctx ; \uctx ; \lctx \seq A}
    \and
    \infer[{\rlab{{\llor}}[2]}]{\tctx ; \uctx ; \lctx \seq A \llor B}{
      \tctx ; \uctx ; \lctx \seq B}
    \and
    \infer[\llab{{\llor}}]{\tctx ; \uctx ; \lmatch[']{A \llor B} \seq C}{
      \tctx ; \uctx ; \lfill[']{A} \seq C &
      \tctx ; \uctx ; \lfill[']{B} \seq C}
    \\
    \infer[\rlab{\forall}]{\tctx ; \uctx ; \lctx \seq \univq x:\tau. A_x}{
      \tctx, x{:}\tau ; \uctx ; \lctx \seq A_x}
    \and
    \infer[\llab{\forall}]{\tctx ; \uctx ; \lmatch[']{\univq x:\tau. A_x} \seq C}{
      \tctx \seq M : \tau &
      \tctx ; \uctx ; \lfill[']{\subst{M/x}{A_x}} \seq C}
    \\
    \infer[\rlab{\exists}]{\tctx ; \uctx ; \lctx \seq \existq x:\tau. A_x}{
      \tctx \seq M : \tau &
      \tctx ; \uctx ; \lctx \seq \subst{M/x}{A_x}}
    \and
    \infer[\llab{\exists}]{\tctx ; \uctx ; \lmatch[']{\existq x:\tau. A_x} \seq C}{
      \tctx, x{:}\tau ; \uctx ; \lfill[']{A_x} \seq C}
    % \\
    % \infer[\rlab{\bang}]{\tctx ; \uctx ; \lctxe \seq \bang A}{
    %   \tctx ; \uctx ; \lctxe \seq A}
    % \and
    % \infer[\llab{\bang}]{\tctx ; \uctx ; \lmatch[']{\bang A} \seq C}{
    %   \tctx ; \uctx, A ; \lfill[']{} \seq C}
    \\
    \infer[\lab{copy}]{\tctx ; \uctx, A ; \lctx' \seq C}{
      \tctx ; \uctx, A ; \lctx', A \seq C}
    \and
    \infer[\lab{cut$^{\bang}_A$}]{\tctx ; \uctx ; \lctx' \seq C}{
      \tctx ; \uctx ; \lctxe \seq A &
      \tctx ; \uctx, A ; \lctx' \seq C}
  \end{mathpar}
  \caption{A sequent calculus for a fragment of first-order \acl{ILL}.\label{fig:seq-jill}}
\end{figure}

\subsection{Proof search as computation: Bottom-up linear logic programming}\label{sec:linear-lp}

% \NewDocumentCommand{\lfoc}{m}{[#1]}%
% \NewDocumentCommand{\rfoc}{m}{[#1]}%
% \begin{figure}
%   \begin{mathpar}
%     \infer[\lab{id$_{P^+}$}]{\uctx ; P^+ \seq \rfoc{P^+}}{
%       }
%     \\
%     \infer[\lab{lfoc}]{\uctx ; \lmatch[']{A^-} \seq J}{
%       (\uctx ; \lfill[']{A^-} \seq J \text{\ stable}) &
%       \uctx ; \lfill[']{\lfoc{A^-}} \seq J}
%     \and
%     \infer[\lab{blur}]{\uctx ; \lctx \seq \rfoc{A^-}}{
%       \uctx ; \lctx \seq A^-}
%     \\
%     \infer[\rlab{{\lolli}}]{\uctx ; \lctx \seq A^+ \lolli B^-}{
%       \uctx ; \lctx, A^+ \seq B^-}
%     \and
%     \infer[\llab{{\lolli}}]{\uctx ; \lmatch[']{\lctx'_A, \lfoc{A^+ \lolli B^-}} \seq J}{
%       \uctx ; \lctx'_A \seq \rfoc{A^+} &
%       \uctx ; \lfill[']{\lfoc{B^-}} \seq J}
%     \\
%     \infer[\rlab{{\tensor}}]{\uctx ; \lctx_1, \lctx_2 \seq \rfoc{A^+ \tensor B^+}}{
%       \uctx ; \lctx_1 \seq \rfoc{A^+} &
%       \uctx ; \lctx_2 \seq \rfoc{B^+}}
%     \and
%     \infer[\llab{{\tensor}}]{\uctx ; \lmatch[']{A^+ \tensor B^+} \seq J}{
%       \uctx ; \lfill[']{A^+, B^+} \seq J}
%     \\
%     \infer[\rlab{\one}]{\uctx ; \lctxe \seq \rfoc{\one}}{
%       }
%     \and
%     \infer[\llab{\one}]{\uctx ; \lmatch[']{\one} \seq J}{
%       \uctx ; \lfill[']{} \seq J}
%     \\
%     \infer[\rlab{{\with}}]{\uctx ; \lctx \seq A^- \with B^-}{
%       \uctx ; \lctx \seq A^- &
%       \uctx ; \lctx \seq B^-}
%     \and
%     \infer[{\llab{{\with}}[1]}]{\uctx ; \lmatch[']{\lfoc{A^- \with B^-}} \seq J}{
%       \uctx ; \lfill[']{\lfoc{A^-}} \seq J}
%     \and
%     \infer[{\llab{{\with}}[2]}]{\uctx ; \lmatch[']{\lfoc{A^- \with B^-}} \seq J}{
%       \uctx ; \lfill[']{\lfoc{B^-}} \seq J}
%     \\
%     \infer[\lab{copy}]{\uctx, A^- ; \lmatch[']{} \seq J}{
%       (\uctx, A^- ; \lfill[']{} \seq J \text{\ stable}) &
%       \uctx, A^- ; \lfill[']{\lfoc{A^-}} \seq J}
%     \\
%     \infer[\lab{lax}]{\uctx ; \lctx \seq A^+ \lax}{
%       \uctx ; \lctx \seq \rfoc{A^+}}
%     \and
%     \infer[\rlab{\monad{}}]{\uctx ; \lctx \seq \monad{A^+}}{
%       \uctx ; \lctx \seq A^+ \lax}
%     \and
%     \infer[\llab{\monad{}}]{\uctx ; \lmatch[']{\lfoc{\monad{A^+}}} \seq C^+ \lax}{
%       \uctx ; \lfill[']{A^+} \seq C^+ \lax}
%   \end{mathpar}
%   \caption{A focused sequent calculus for linear lax logic.\label{fig:focused-linear-logic}}
% \end{figure}%

\begin{itemize}
\item Notion of transition obtained by reading sequent calculus left rules  (bipoles) bottom-up.
\end{itemize}

\subsection{Proof reduction as computation: Session-typed linear logic}\label{sec:async-sill}

Having seen a proof-reduction-as-computation correspondence between intuitionistic logic and functional computation (\cref{sec:proof-rec-as-comp}), it's natural to ask if there is a similar computational interpretation of the intuitionistic linear sequent calculus.
% as session-type discipline for concurrent processes:
% Giving session-typed concurrency a logical footing,
\Textcite{Caires+Pfenning:CONCUR10} along with Toninho~\autocite*{Caires+:TLDI12} have developed
% a Curry-Howard
an interpretation
% of the intuitionistic linear sequent calculus
in which proofs are processes, propositions are session types, and proof reduction is synchronous interprocess communication.

\Textcite{Toninho+:ESOP13} later extended this interpretation with recursive session types and integrated it into a functional language.
Here we review that integration, with one modification:
Instead of interpreting proof reductions as synchronous communication, we give an asynchronous operational semantics that is better suited to the proposed thesis, as we will see in \cref{sec:compile}, while still remaining faithful to the proof reductions~\autocite{DeYoung+:CSL12}.

\subsubsection{Session-typing judgment}\label{sec:sess-typing-judgm}

In a session-based model of concurrency, pairs of processes interact in well-defined sessions, with one process offering a service that its session partner uses.
Session types, pioneered by \textcite{Honda:CONCUR93}, describe the interaction protocol to which\fxnote{\ \st{a process adheres when offering its service}} the processes in that session must adhere.
% When processes interact according to the protocol, their states change
When processes interact, the session type changes: one process now offers, and the other uses, the continuation of the initial service.
The logical reading of session-based concurrency is linear logic exactly because it can express this change of state.
% \Citeauthor{Caires+:TLDI12}'s logical reading of session-based concurrency is as linear logic exactly because it can express this change of state.
% % It's for this reason that session-based concurrency is a computational interpretation of linear logic.

Because a process offers its service along a distinguished channel, the basic session-typing judgment is $P :: c{:}A$, meaning \enquote{process $P$ offers service of type $A$ along channel $c$}.
However, $P$ itself may rely on services offered by yet other processes, and so our session-typing judgment is, more generally, a linear sequent annotated as
\begin{equation*}
  \underbrace{c_1{:}A_1, \dotsc, c_n{:}A_n}_{\displaystyle \lctx} \seq P :: c{:}A \:,
\end{equation*}
meaning \enquote{Using services $A_i$ offered along channels $c_i$, the process $P$ offers service $A$ along channel $c$.}
The channels $c_i$ and $c$ must all be distinct and are binding occurrences with scope over the process $P$.

With this session-typing judgment, the inference rules of the linear sequent calculus become session-typing rules for concurrent processes.

% \fxerror{Recall from \cref{sec:judgmental-rules} that the sequent calculus for \ac{JILL} includes cut and identity rules that relate uses of resources to plans for obtaining resources.}

\subsubsection{Cut as composition}\label{sec:cut-as-composition}

Recall from \cref{sec:judgmental-rules} that the cut rule
% for \ac{JILL} 
composes a plan for obtaining resource $A$ with another plan that uses resource $A$:
\begin{equation*}
  \infer[\lab{cut$_A$}]{\lmatch[']{\lctx} \seq C}{
    \lctx \seq A &
    \lfill[']{A} \seq C}
  \:.
\end{equation*}
Because proofs are to be processes, this suggests that the process interpretation of the cut rule should compose a process that offers service $A$ with another process that uses service $A$.
The $\lab{cut$_A$}$ rule thus becomes a typing rule for process composition:
% annotated rule is thus
\begin{equation*}
  \infer[\lab{cut$_A$}]{\lctx, \lctx' \seq \mbind{\bv{c} <- \mspawn{P} <- \lctx; Q_{\bv{c}}} :: e{:}C}{
    \lctx \seq P :: c{:}A &
    \lctx', c{:}A \seq Q_c :: e{:}C}
  \:,
\end{equation*}
where $\mbind{\bv{c} <- \mspawn{P} <- \lctx; Q_{\bv{c}}}$ means \enquote{Spawn process $P$ that uses channels $\lctx$ to offer service along a fresh channel $c$, and then continue as process $Q_c$.}
The syntax is reminiscent of \haskell`do` notation for monadic computations, with the channel $\bv{c}$ being bound within $\mbind{\bv{c} <- \mspawn{P} <- \lctx; Q_{\bv{c}}}$.

\NewPredicate{\exec}{1}
To complete the description of $\mspawn{}$, we must make its operational semantics precise.
Rather than using \iacl{SOS}, it is convenient to describe the semantics as a forward-chaining linear logic program \autocites{Cervesato+:CMU02}{Pfenning:APLAS04}, in the style known as \iac{SSOS}.
In our \ac{SSOS}, we will use the linear proposition $\exec{P}$ to represent an executing process $P$.
The rule for executing a $\mspawn{}$ is
\begin{equation*}
  \exec{(\mbind{\bv{c} <- \mspawn{P} <- \lctx; Q_{\bv{c}}})}
    \lolli \monad{\existq c. \exec{P} \tensor \exec{Q_c}}
  \:.
\end{equation*}
Thus, to execute the $\mspawn{}$, we create a fresh channel $c$ and then execute the 
% offering process, $P$, and its client process, $Q_c$, in parallel.
process $P$, which offers a service along $c$, in parallel with its client process, $Q_c$.


\subsubsection{Additive conjunction as branching}\label{sec:addit-conj-as-branching}

\NewDocumentCommand{\inj}{m}{\mathtt{in}#1}
\NewDocumentCommand{\inl}{}{\inj{\mathtt{l}}}
\NewDocumentCommand{\inr}{}{\inj{\mathtt{r}}}

So far we have discussed only a judgmental principle that applies to all services; specific services are defined by the right and left rules of the logical connectives.

% \paragraph{Static semantics.}\label{sec:with-static-semantics}

Recall from \cref{sec:prop-conn} that the resource $A \with B$ gives a choice of resources $A$ and $B$.
The right rule for $A \with B$ was
\begin{equation*}
  \infer[\rlab{{\with}}]{\lctx \seq A \with B}{
    \lctx \seq A &
    \lctx \seq B}
  \:.
\end{equation*}
It says that to prove $A \with B$ we must prove both $A$ and $B$\fxnote{\ \st{(using the same resources $\lctx$)}} so that we are prepared for whichever of these two resources is eventually chosen.
%
Correspondingly, a process that offers service $A \with B$ gives its client a choice of services $A$ and $B$; the process must be prepared to offer whichever service the client chooses.
Based on this intuition, we interpret the~$\rlab{{\with}}$ rule as typing a binary guarded choice:
% Based on this intuition, we assign a binary guarded choice process to the $\rlab{{\with}}$ rule:
\begin{equation*}
  \infer[\rlab{{\with}}]{\lctx \seq \mcase{c}{\inl => P_1}{\inr => P_2} :: c{:}A \with B}{
    \lctx \seq P_1 :: c{:}A &
    \lctx \seq P_2 :: c{:}B}
  \:,
\end{equation*}
where $\mcase{c}{\inl => P_1}{\inr => P_2}$ means \enquote{Input either $\inl$ or $\inr$ along channel $c$, and then continue as process $P_1$ or $P_2$, respectively.}

Conversely, the client that uses service $A \with B$ must behave in a complementary way:
it should select either service $A$ or service $B$ and then, having notified the offering process of its choice (as $\inl$ or $\inr$), continue the session by using that service.
The left rules for type $A \with B$ are thus
\begin{mathpar}
  \infer[{\llab{{\with}}[1]}]{\lmatch[']{c{:}A \with B} \seq \moutputl{c.\inl; Q} :: e{:}C}{
    \lfill[']{c{:}A} \seq Q :: e{:}C}
  \and
  \infer[{\llab{{\with}}[2]}]{\lmatch[']{c{:}A \with B} \seq \moutputl{c.\inr; Q} :: e{:}C}{
    \lfill[']{c{:}B} \seq Q :: e{:}C}
  \:,
\end{mathpar}
where $\moutputl{c.\inj{\mathtt{l/r}}; Q}$ means \enquote{Send label $\inj{\mathtt{l/r}}$ along channel $c$ and then continue as process $Q$.}

% \paragraph{Operational semantics.}\label{sec:with-oper-semant}

Our intuition about the behavior of the guarded choice processes is made precise by their operational semantics.
First, the $\inl$ branch:
\NewPredicate{\msgc}[msg_c]{3}
\NewPredicate{\msgl}[msg_l]{3}
\begin{align*}
  &\exec{(\moutputl{c.\inl; Q})}
     \lolli \monad{\existq c'. \msgl{c,\inl,c'} \tensor \exec{(\subst{c'/c}{Q})}} \\
  %
  &\exec{(\mcase{c}{\inl => P_1}{\inr => P_2})} \tensor \msgl{c,\inl,c'}
     \lolli \monad{\exec{(\subst{c'/c}{P_1})}}
  \:.
\end{align*}
To execute the selection process $\moutputl{c.\inl; Q}$,
% we first send label $\inl$ along channel $c$, with a fresh channel $c'$ designated as the channel along which the session should continue.
we asynchronously send along channel $c$ a message containing label $\inl$ and a fresh channel $c'$,
% designated as the channel along which the session should continue
which is represented in the \ac{SSOS} as the proposition $\msgl{c,\inl,c'}$, and then immediately continue\fxnote{\ \st{the session}}
by executing $\subst{c'/c}Q$, \ie\ process $Q$ with channel $c'$ substituted for $c$.
When this message arrives, the destination process $\mcase{c}{\inl => P_1}{\inr => P_2}$ resumes execution as $\subst{c'/c}{P_1}$.
In this way, $c'$ is a fresh channel at which the offering process and its client rendezvous for the session continuation.
% This message is represented as the linear propostion $\msgl{c,\inl,c'}$.

If, instead of using a fresh channel, $c$ was reused as the session continuation channel, then consecutive selections along $c$ could be received out of order.
Because the communication is asynchronous, there would be no way to distinguish whether, for instance, $\msgl{c,\inl,\!}$ or $\msgl{c,\inr,\!}$ was sent first.
But, by sending the second message over a fresh continuation channel, the order is clear~\autocite{DeYoung+:CSL12}: $\msgl{c,\inl,c'}$ was sent before $\msgl{c',\inr,c''}$.

Although the operational semantics relies on fresh continuation channels, notice that the \emph{syntax} hides them from the programmer (hence the substitutions $\subst{c'/c}$ that appear in the semantics).
It would also be possible to explicitly incorporate continuation channels in the syntax (and thereby eliminate the substitutions), at the expense of readability, however.

The operational semantics of the $\inr$ branch is symmetric to that of the $\inl$ branch:
\begin{align*}
  &\exec{(\moutputl{c.\inr; Q})}
     \lolli \monad{\existq c'. \msgl{c,\inr,c'} \tensor \exec{(\subst{c'/c}{Q})}} \\
  %
  &\exec{(\mcase{c}{\inl => P_1}{\inr => P_2})} \tensor \msgl{c,\inr,c'}
     \lolli \monad{\exec{(\subst{c'/c}{P_2})}}
  \:.
\end{align*}

\paragraph{Practical considerations.}\label{sec:with-pract-cons}

To make the language more palatable for the programmer, we diverge slightly from a pure propositions-as-types interpretation of linear logic by including $n$-ary \emph{labeled} additive conjunctions $\nwith[_i]{{\ell_i}{A_i}}$ as a primitive.
The static and operational semantics are thus:
\begin{gather*}
  \infer[\rlab{{\with}}]{\lctx \seq \mcase{c}{\ell_i => P_i} :: c{:}\nwith[_i]{{\ell_i}{A_i}}}{
    \forall i\mathpunct{:}\;\;  \lctx \seq P_i :: c{:}A_i}
  \\[\jot]
  \infer[\llab{{\with}}]{\lmatch[']{c : \nwith[_i]{{\ell_i}{A_i}}} \seq \moutputl{c.\ell_k; Q} :: e{:}C}{
    \lfill[']{c{:}A_k} \seq Q :: e{:}C}
  \\[2\jot]
  \!\begin{aligned}
    &\exec{(\moutputl{c.\ell_k; Q})}
       \lolli \monad{\existq c'. \msgl{c,\ell_k,c'} \tensor \exec{(\subst{c'/c}{Q})}}
    \\
    &\exec{(\mcase{c}{\ell_i => P_i})} \tensor \msgl{c,\ell_k,c'}
       \lolli \monad{\exec{(\subst{c'/c}{P_k})}}
  \end{aligned}
\end{gather*}
Another possibility would be to simply treat $n$-ary labeled conjunctions as syntactic sugar for nested binary conjunctions, but this would turn out to introduce a communication overhead because we would be sending multiple $\inl$/$\inr$s separately rather than a single label $\ell_k$.


\subsubsection{Recursive session types and process definitions}\label{sec:recurs-sess-types}

Concurrent processes frequently exhibit unbounded or infinite, yet well-defined, behavior;
for instance, we may wish to have a counter that offers an increment service indefinitely.
To this end, \textcite{Toninho+:coind13} are currently extending the \ac{SILL} type theory with inductive and coinductive session types.
Meanwhile, however, we must content ourselves to depart from a pure Curry-Howard correspondence and instead rely on general recursion.

Session types thus include general recursive types, $\mu t.A$, and type variables, $t$.
The type $\mu t.A$ is interpreted equi-recursively, being identified with its unfolding, namely $\subst{(\mu t.A)/t}{A}$.
Processes correspondingly include mutually recursive process definitions, via \msill`letrec`, and process variables, $X$.

\let\pctx\tctx
We
% first step is to introduce process variables, $X$, and 
extend the session-typing judgment with a context, $\pctx$, of process variable typings.
% Because a process is typed according to the services that it uses and offers, process variables are typed as $X : \ctxmonad{A <- \vec{B}}$, meaning that process $X$ can offer service $A$ if provided with channels along which services $\vec{B}$ are offered.
Because a process is typed according with a sequent of services that it uses and offers, process variables are typed as ${X : \ctxmonad{A <- \vec{B}}}$ if process $X$ can offer service $A$ 
% when provided with channels along which services $\vec{B}$ are offered.
by using services $\vec{B}$.
When channels of appropriate types are available, the process $X$ can be called:
%  If channels of the types in $\lctx$ are available, then process $X$ can be called with those channels.
\begin{equation*}
  \infer[\lab{call}]{\tctx, X{:}\ctxmonad{A <- \vec{B}} ; \lctx \seq \mbind{c <- X <- \vec{d}} :: c{:}A}{
    \lctx = d_1{:}B_1, \dotsc, d_n{:}B_n}
  \:.
\end{equation*}
A common idiom is $\mbind{\bv{c} <- \mspawn{(\mbind{\bv{c} <- X <- \vec{d}})} <- \vec{d}; Q_{\bv{c}}}$, which spawns a call to $X$ that is run in parallel with some process $Q_c$.
We will frequently abbreviate this with the syntactic sugar $\mbind{\bv{c} <- X <- \vec{d}; Q_{\bv{c}}}$.

% Mutually recursive process definitions add process variables to the context.
% Process variables are added to the context to allow for mutual recursion.
In mutually recursive process definitions,
% , the programmer declares each process with a type that must be checked.
the process bodies may refer to any of the mutually recursive processes via process variables.
The typing rule is
\begin{gather*}
  \infer[\lab{letrec}]{\pctx ; \lctx \seq \mletrec{\mprocdef{c_1 <- X_1 <- \vec{d_1} = P_1} and \dotsb and \mprocdef{c_n <- X_n <- \vec{d_n} = P_n}}{Q} :: c{:}C}{
    \forall i\mathpunct{:}\;\; \pctx' ; d_{i,1}{:}B_{i,1}, \dotsc, d_{i,k_i}{:}B_{i,k_i} \seq P_i :: c_i{:}A_i &&
    \pctx' ; \lctx \seq Q :: c{:}C} \\
\shortintertext{where}
  \pctx' = \pctx, X_1{:}\ctxmonad{A_1 <- \vec{B_1}}, \dotsc, X_n{:}\ctxmonad{A_n <- \vec{B_n}}
  \:.
\end{gather*}

Now, having presented recursion, we can finally give a simple example program.


\subsubsection{Example: Binary counter}\label{sec:exampl-binary-count}

% As an example session-typed program, we can implement a simple counter as shown in \cref{fig:counter-inc}.%
We can implement a simple session-typed counter on natural numbers as shown in \cref{fig:counter-inc}.%
\footnote{This example is adapted from one by \textcite{Toninho+:ESOP13}.}
%
\begin{sillcode}[
  caption={A simple binary counter supporting an increment operation},%
  label={fig:counter-inc},%
  gobble=2
]
  stype Counter = &{ inc: Counter }
  
  eps : {|- c:Counter}
  c <- eps =
  { case c of
      inc => d <- eps;
             c <- bit1 <- d }
  
  bit0 : {d:Counter |- c:Counter}
  c <- bit0 <- d =
  { case c of
      inc => c <- bit1 <- d }
  
  bit1 : {d:Counter |- c:Counter}
  c <- bit1 <- d =
  { case c of
      inc => d.inc;
             c <- bit0 <- d }
\end{sillcode}
%
The counter is a linear network of \msill`bit0` and \msill`bit1` processes, one for each bit in the binary representation of the counter's value, and is terminated at the most significant end with an \msill`eps` process.
For instance, the process network \msill`<- bit0 <- bit1 <- eps`\fxnote{\ [Write in opposite order?]} represents a counter with value $2$.

The counter offers a very simple service: the client may only choose to increment the counter, with the same service being offered recursively after the increment.
% The counter thus offers a service of type \msill`Counter` declared as \msill`stype Counter = &{ inc: Counter }`.
This service, \msill`Counter`, is therefore a recursive additive conjunction,
% The service is of type \msill`Counter`,
declared in the concrete syntax as \msill`stype Counter = &{ inc: Counter }`.
The \msill`eps` process offers this service outright, and thus has type \msill`{|- Counter}`.
% , whereas the \msill`bit0` and \msill`bit1` processes use the service offered by their more significant neighbors.
The \msill`bit0` and \msill`bit1` processes, on the other hand, use the service offered by their more significant neighbors, and thus have type \msill`{Counter |- Counter}`.

The process definitions of \msill`eps`, \msill`bit0`, and \msill`bit1` are mutually recursive.
When an \msill`eps` process receives an \msill`inc` message, it creates a new most significant bit by spawning a new \msill`eps` process and then making a recursive call to a \msill`bit1` process.
% that uses the service offered by the new \msill`eps`.
When a \msill`bit0` process receives an \msill`inc`, the bit is flipped by way of a recursive call to a \msill`bit1` process.
Lastly, when a \msill`bit1` process receives an \msill`inc`, the bit is flipped and a carry is propagated; this is accomplished by first sending \msill`inc` along channel \msill`d` to the \msill`Counter` offered by the next more significant bit and then making a recursive call to a \msill`bit0` process.

Informally, we can see that, as implemented, the \msill`inc` operation respects a counter's denotation: whenever a counter representing natural number $n$ is incremented, the resulting counter represents $n+1$.
Note, however, that this adequacy property is not enforced by the type \msill`Counter`.
An appropriate dependent session type could enforce increment adequacy, but, for simplicity of presentation, we prefer the simple type for now.


\subsubsection{Additive disjunction as choice}\label{sec:addit-disj-as-choice}

In the intuitionistic linear sequent calculus, additive disjunction, $A \llor B$, is dual to additive conjunction, $A \with B$.
We should expect this duality to also appear in the process assignment.
% Because the additive disjunction $A \llor B$ and additive conjunction $A \with B$ are dual, their process assignments are also dual:
Whereas a process of type $A \with B$ offers its client a choice of services $A$ and $B$, a process of type $A \llor B$ chooses between offering service $A$ or service $B$ to its client.
The client waits to be notified of the offering process's choice and then uses that service.
\begin{mathpar}
  \infer[{\rlab{{\llor}}[1]}]{\lctx \seq \moutputl{c.\inl; P} :: c{:}A \llor B}{
    \lctx \seq P :: c{:}A}
  \and
  \infer[{\rlab{{\llor}}[2]}]{\lctx \seq \moutputl{c.\inr; P} :: c{:}A \llor B}{
    \lctx \seq P :: c{:}B}
  \and
  \infer[\llab{{\llor}}]{\lmatch[']{c{:}A \llor B} \seq \mcase{c}{\inl => Q_1}{\inr => Q_2} :: e{:}C}{
    \lmatch[']{c{:}A} \seq Q_1 :: e{:}C &
    \lmatch[']{c{:}B} \seq Q_2 :: e{:}C}
\end{mathpar}
Once again, to make the language more convenient for the programmer, we include $n$-ary labeled additive disjunctions $\nllor[_i]{{\ell_i}{A_i}}$.
The typing rules are thus more generally
\begin{mathpar}
  \infer[\rlab{{\llor}}]{\lctx \seq \moutputl{c.\ell_k; P} :: c : \nllor[_i]{{\ell_i}{A_i}}}{
    \lctx \seq P :: c{:}A_k}
  \and
  \infer[\llab{{\llor}}]{\lmatch[']{c : \nllor[_i]{{\ell_i}{A_i}}} \seq \mcase{c}{\ell_i => Q_i} :: e{:}C}{
    \forall i\mathpunct{:}\;\;  \lmatch[']{c{:}A_i} \seq Q_i :: e{:}C}
  \mathrlap{\:.}
\end{mathpar}
The operational semantics for these guarded choice constructs was already given in \cref{sec:addit-conj-as-branching}.


\subsubsection{Example: Binary counter with decrements}\label{sec:exampl-binary-count-1}

\Cref{lst:counter-inc-dec} shows a counter that takes advantage of additive disjunction to support a truncated decrement operation.
%
\begin{sillcode}[
  caption={A binary counter supporting increments and decrements},
  label={lst:counter-inc-dec},
  floatplacement=tb,
  gobble=2
]
  stype Counter = &{ inc: Counter ,
                     dec: +{ zero: Counter , succ: Counter } }
  
  eps : {|- c:Counter}
  c <- eps =
  { case c of
      inc => d <- eps;
             c <- bit1 <- d
    | dec => c.zero;
             c <- eps }
  
  bit0 : {d:Counter |- c:Counter}
  c <- bit0 <- d =
  { case c of
      inc => c <- bit1 <- d
    | dec => d.dec;
             (case d of
                zero => c.zero;
                        c <- bit0 <- d
              | succ => c.succ;
                        c <- bit1 <- d) }
  
  bit1 : {d:Counter |- c:Counter}
  c <- bit1 <- d =
  { case c of
      inc => d.inc;
             c <- bit0 <- d
    | dec => c.succ;
             c <- bit0 <- d }
\end{sillcode}%
%
According to the type declaration, a process offering the \msill`Counter` service gives its client a choice of increment or decrement services.
If the client chooses to decrement, the offering process will choose to reply with either \msill`zero` or \msill`succ` and then recursively offer the \msill`Counter` service.
% The type is thus declared as
% \begin{sillcode*}[xleftmargin=\parindent, gobble=2]
%   stype Counter = &{ inc: Counter ,
%                      dec: +{ zero: Counter , succ: Counter } }
% \end{sillcode*}

As implemented, decrementing the counter gives \msill`zero` and leaves the process network unchanged if the counter represents $0$;
if it represents some $N > 0$, then decrementing the counter gives \msill`succ` after decrementing to $N - 1$.
Once again, these adequacy properties are not enforced by the type \msill`Counter`, although they could be with an appropriate dependent session type.

\subsubsection{Identity as forwarding}\label{sec:ident-as-forw}

Also recall the identity principle for \ac{JILL}, which states that one way to obtain a resource is to directly use an existing resource.
Under the process interpretation, a process can offer service $A$ by acting as an forwarding intermediary between its clients and another process that offers service $A$.
The $\lab{id$_A$}$ rule thus types a forwarding process between two channels:
\begin{equation*}
  \infer[\lab{id$_A$}]{d{:}A \seq \mfwd{c <- d} :: c{:}A}{
    }
  \:.
\end{equation*}
Rather than making the forwarding explicit in the operational semantics, we can simply equate the two channels:
\begin{equation*}
  \exec{(\mfwd{c <- d})} \lolli \monad{c \eq d}
\end{equation*}


\subsubsection{Other session types}\label{sec:lolli-as-input-tensor-as-output}

In addition to the session types already mentioned, \ac{SILL} includes session types corresponding to the other connectives of linear logic.
% In addition to processes that send and receive labels and are typed with additive conjunctions and disjunctions, \ac{SILL} supports processes that exchange functional values.

The first-order quantifiers type processes that exchange functional values.
% These are typed by the rules for first-order quantifiers.
A process offering service $\univq x:\tau. A_x$ (or using service $\existq x:\tau. A_x$) first inputs a value $x$ of functional type $\tau$ and then offers (resp., uses) service $A_x$.
% , where $A_x$ may depend on $x$.
Dually, a process offering service $\existq x:\tau. A_x$ (or using service $\univq x:\tau. A_x$) asynchronously outputs the value of some functional term $M$\fxnote{\ \st{a functional value}} of type $\tau$ and then offers (resp., uses) service $\subst{M/x}{A_x}$.
The first order quantifiers are thus dependent session types\fxnote{
\st{; in the non-dependent case, we write the types as $\tau \imp A$ and $\tau \land A$}}.
% Processes that send and receive funcitonal values are written as $\minput{x <- input c; P}$ and $\moutputv{c}{M}{Q}$.
The syntax for value inputs and outputs is $\minput{\bv{x} <- input c; P_{\bv{x}}}$ and $\moutputv{c}{M}{Q}$.

Linear implications and multiplicative conjunctions type processes that send and receive channels.
% \Ac{SILL} also includes linear implication and multiplicative conjunction as session types.
A process offering service $A \lolli B$ (or using service $A \tensor B$) first inputs a channel of type $A$ and then offers (resp., uses) service $B$.
Dually, a process offering service $A \tensor B$ (or using service $A \lolli B$) asynchronously outputs a fresh channel of type $A$ and then offers (resp., uses) service $B$.
% Similarly to the quantifiers, the left and right rules for linear implication and multiplicative conjunction thus type channel inputs and outputs, which are written as $\minput{d <- input c; P}$ and $\moutputc{c}{d <- P}{Q}$.
The syntax for channel inputs and outputs is $\minput{\bv{d} <- input c; P_{\bv{d}}}$ and $\moutputc{c}{\bv{d} <- Q_{\bv{d}}}{R}$.

Since value and channel inputs and outputs are not important to the remainder of this proposal, the reader who is interested in further details of their static and operational semantics should refer to the papers by \textcites{Toninho+:ESOP13}{Toninho+:PPDP11}.


\subsection{Ordered logic}\label{sec:ordered-logic}

As described in \cref{sec:introduction}, this thesis proposes to show that session types form a bridge between notions of concurrency that arise in proof-search-as-computation and proof-reduction-as-computation interpretations of linear logic.
Unfortunately, proof search in first-order linear logic appears to be a bit too general to tackle outright.

In this proposal, we therefore turn our attention to the special case of propositional ordered logic~\autocites{Lambek:AMM58}{Polakow+Pfenning:MFPS99}.
% leaving the details of the more general case as thesis research.
% In linear logic, the order of hypotheses is unimportant; $A \tensor B \lolli B \tensor A$ is provable, for example.
% Pure ordered logic, on the other hand, is a restriction of linear logic in which order matters:
% in addition to rejecting contraction and weakening, ordered logic also rejects the structural principle of exchange.
In linear logic, the order of hypotheses is always unimportant---$A \tensor B \lolli B \tensor A$ is provable, for example---but in ordered logic the order matters.

Nevertheless, \textcite{Simmons+Pfenning:HOSC11} have shown that there is a sound and complete translation from ordered logic to first-order linear logic with equality.
This justifies taking ordered logic as a special case of linear logic.
An understanding of concurrency in ordered logic will thus be invaluable to the thesis goal of understanding how session types bridge the notions of concurrency in linear logic.

Once again, the reader who is familiar with ordered logic may easily skip the remainder of this \lcnamecref{sec:ordered-logic}.

\subsubsection{Sequent calculus}\label{sec:sequent-calculus}

In addition to rejecting contraction and weakening like linear logic, ordered logic also rejects the structural principle of exchange.
The basic hypothetical judgment in the sequent calculus for propositional ordered lax logic is thus
\begin{equation*}
  \underbrace{A_1, \dotsc, A_n}_{\displaystyle \octx} \seq A \:,
\end{equation*}
which states that resource $A$ can be obtained by using resources $A_1, \dotsc, A_n$ \emph{in order}.
% Because there is no exchange principle, 
The ordered context $\octx$ is therefore treated as a list (\ie, a non-commutative monoid) of resources.
To this basic hypothetical judgement, we add a persistant context $\uctx$ holding persistant facts, just as we did in linear logic.

Having rejected exchange, we must reconsider the various forms of proposition and judgmental principles.
\begin{itemize}
\item Like linear logic, ordered logic contains a multiplicative conjunction in which both conjuncts must be used.
      However, unlike $A \tensor B$, it cannot be commutative because the logic rejects exchange.
      We write this ordered conjunction as $A \fuse B$ to make the distinction clear.
%
\item
% In place of $A \lolli B$, Ordered logic has left- and right-handed implications $A \limp B$ and $A \rimp B$: without exchange, it now matters from which side resources are used to obtain the implication's antecedent.
      Without exchange, it now matters from which side resources are used to obtain an implication's antecedent.
      In ordered logic, implication is thus divided into left- and right-handed forms, $A \limp B$ and $A \rimp B$: 
      when using a left (or right) implication, only resources to the immediate left (resp., right) may be used.
% ; when using a right implication, only resources to the immediate right may be used.
%
\item The other propositions---additive conjunction, multiplicative unit, and the lax modality---are unchanged from linear logic because they do not rely on the exchange principle.%
      \footnote{An additive disjunction could also be included in ordered logic, but it is not needed for this proposal.}
%
\item The identity and cut principles for ordered logic are essentially the same as in linear logic, except that they are now stated in a way that avoids assuming an exchange principle.
\end{itemize}

The sequent calculus rules for propositional ordered lax logic are given in \cref{fig:ordered-logic}.
\NewDocumentCommand{\ofrm}{O{}}{\Theta#1}%
\NewDocumentCommand{\omatch}{O{} m}{\ofrm[#1]\{#2\}}%
\NewDocumentCommand{\ofill}{O{} m}{\ofrm[#1]\{#2\}}%
\begin{figure}
  \begin{mathpar}
    \infer[\lab{id$_A$}]{\uctx ; A \seq A}{
      }
    \and
    \infer[\lab{cut$_A$}]{\uctx ; \omatch{\octx} \seq J}{
      \uctx ; \octx \seq A &
      \uctx ; \ofill{A} \seq J}
    \\
    \infer[\rlab{{\limp}}]{\uctx ; \octx \seq A \limp B}{
      \uctx ; A, \octx \seq B}
    \and
    \infer[\llab{{\limp}}]{\uctx ; \omatch{\octx', A \limp B} \seq J}{
      \uctx ; \octx' \seq A &
      \uctx ; \ofill{B} \seq J}
    \\
    \infer[\rlab{{\rimp}}]{\uctx ; \octx \seq A \rimp B}{
      \uctx ; \octx, A \seq B}
    \and
    \infer[\llab{{\rimp}}]{\uctx ; \omatch{A \rimp B, \octx'} \seq J}{
      \uctx ; \octx' \seq A &
      \uctx ; \ofill{B} \seq J}
    \\
    \infer[\rlab{{\fuse}}]{\uctx ; \octx_1, \octx_2 \seq A \fuse B}{
      \uctx ; \octx_1 \seq A &
      \uctx ; \octx_2 \seq B}
    \and
    \infer[\llab{{\fuse}}]{\uctx ; \omatch{A \fuse B} \seq J}{
      \uctx ; \ofill{A, B} \seq J}
    \\
    \infer[\rlab{\one}]{\uctx ; \octxe \seq \one}{
      }
    \and
    \infer[\llab{\one}]{\uctx ; \omatch{\one} \seq J}{
      \uctx ; \ofill{\octxe} \seq J}
    \\
    \infer[\rlab{{\with}}]{\uctx ; \octx \seq A \with B}{
      \uctx ; \octx \seq A &
      \uctx ; \octx \seq B}
    \and
    \infer[{\llab{{\with}}[1]}]{\uctx ; \omatch{A \with B} \seq J}{
      \uctx ; \ofill{A} \seq J}
    \and
    \infer[{\llab{{\with}}[2]}]{\uctx ; \omatch{A \with B} \seq J}{
      \uctx ; \ofill{B} \seq J}
    \\
    \infer[\lab{copy}]{\uctx, A ; \omatch{\octxe} \seq J}{
      \uctx, A ; \ofill{A} \seq J}
    \and
    \infer[\lab{cut$^{\bang}_A$}]{\uctx ; \octx' \seq J}{
      \uctx ; \octxe \seq A &
      \uctx, A ; \octx' \seq J}
    \\
    \infer[\lab{lax}]{\uctx ; \octx \seq A \lax}{
      \uctx ; \octx \seq A}
    \and
    \infer[\lab{cut$^{{\monad[split=false]{}}}_A$}]{\uctx ; \omatch{\octx} \seq C \lax}{
      \uctx ; \octx \seq A \lax &
      \uctx ; \ofill{A} \seq C \lax}
    \\
    \infer[\rlab{\monad{}}]{\uctx ; \octx \seq \monad{A}}{
      \uctx ; \octx \seq A \lax}
    \and
    \infer[\llab{\monad{}}]{\uctx ; \omatch{\monad{A}} \seq C \lax}{
      \uctx ; \ofill{A} \seq C \lax}
  \end{mathpar}
  \caption{A sequent calculus for a fragment of propositional ordered lax logic.\label{fig:ordered-logic}}
\end{figure}%
Following \textcite{Simmons:CMU12}, we choose to present the rules using \vocab{frames}.
A frame $\ofrm$ is an ordered context with a single hole that may be filled by another ordered context.
For instance, the left rule for left implication is expressed as 
\begin{equation*}
  \infer[\llab{{\limp}}]{\uctx ; \omatch{\octx'_A, A \limp B} \seq J}{
    \uctx ; \octx'_A \seq A &
    \uctx ; \ofill{B} \seq J}
  \:.
\end{equation*}
Reading bottom-up, the rule may be applied if:
% \begin{enumerate*}
% \item
the ordered context can be matched as some frame $\ofrm$ filled with $\octx'_A, A \limp B$;
% \item
$A$ is provable from the context $\octx'_A$; and
% \item
$J$ is provable from the context consisting of the frame $\ofrm$ filled with $B$.
% \end{enumerate*}


% in the rule's conclusion, the matching construct $\omatch{\octx', A \limp B}$ indicates that the rule applies only if the ordered context can be matched as frame $\ofrm$ filled with $\octx', A \limp B$ for some $\ofrm$ and $\octx'$.
% In the second premise, the construct $\ofill{B}$ indicates that the ordered context is frame $\ofrm$ filled with the singleton context $B$.




\subsection{Concurrent ordered logic programming}\label{sec:ordered-lp}

As we saw in \cref{sec:linear-lp}, proof search in linear logic is the foundation for concurrent linear logic programming, which can be seen as a logically motivated form of multiset rewriting.
Likewise, proof search in ordered logic forms the basis of concurrent ordered logic programming~\autocites{Pfenning+Simmons:LICS09}{Simmons:CMU12}, which is a form of string rewriting~\autocite[see, \eg,][]{Book+Otto:SRS93}.

% A focused sequent calculus for ordered logic is given in \cref{fig:focused-ordered-logic}.
% %
% \begin{figure}
%   \begin{mathpar}
%     \infer[\lab{id$_{P^+}$}]{\uctx ; P^+ \seq \rfoc{P^+}}{
%       }
%     \and
%     \infer[\lab{lfoc}]{\uctx ; \omatch{A^-} \seq J}{
%       (\uctx ; \ofill{A^-} \seq J \text{\ stable}) &
%       \uctx ; \ofill{\lfoc{A^-}} \seq J}
%     \\
%     \infer[\rlab{{\limp}}]{\uctx ; \octx \seq A^+ \limp B^-}{
%       \uctx ; A^+, \octx \seq B^-}
%     \and
%     \infer[\llab{{\limp}}]{\uctx ; \omatch{\octx', \lfoc{A^+ \limp B^-}} \seq J}{
%       \uctx ; \octx' \seq \rfoc{A^+} &
%       \uctx ; \ofill{\lfoc{B^-}} \seq J}
%     \\
%     \infer[\rlab{{\rimp}}]{\uctx ; \octx \seq A^+ \rimp B^-}{
%       \uctx ; \octx, A^+ \seq B^-}
%     \and
%     \infer[\llab{{\rimp}}]{\uctx ; \omatch{\lfoc{A^+ \rimp B^-}, \octx'} \seq J}{
%       \uctx ; \octx' \seq \rfoc{A^+} &
%       \uctx ; \ofill{\lfoc{B^-}} \seq J}
%     \\
%     \infer[\rlab{{\fuse}}]{\uctx ; \octx_1, \octx_2 \seq \rfoc{A^+ \fuse B^+}}{
%       \uctx ; \octx_1 \seq \rfoc{A^+} &
%       \uctx ; \octx_2 \seq \rfoc{B^+}}
%     \and
%     \infer[\llab{{\fuse}}]{\uctx ; \omatch{A^+ \fuse B^+} \seq J}{
%       \uctx ; \ofill{A^+, B^+} \seq J}
%     \\
%     \infer[\rlab{\one}]{\uctx ; \octxe \seq \rfoc{\one}}{
%       }
%     \and
%     \infer[\llab{\one}]{\uctx ; \omatch{\one} \seq J}{
%       \uctx ; \ofill{\octxe} \seq J}
%     \\
%     \infer[\rlab{{\with}}]{\uctx ; \octx \seq A^- \with B^-}{
%       \uctx ; \octx \seq A^- &
%       \uctx ; \octx \seq B^-}
%     \and
%     \infer[{\llab{{\with}}[1]}]{\uctx ; \omatch{\lfoc{A^- \with B^-}} \seq J}{
%       \uctx ; \ofill{\lfoc{A^-}} \seq J}
%     \and
%     \infer[{\llab{{\with}}[2]}]{\uctx ; \omatch{\lfoc{A^- \with B^-}} \seq J}{
%       \uctx ; \ofill{\lfoc{B^-}} \seq J}
%     \\
%     \infer[\lab{copy}]{\uctx, A^- ; \omatch{\octxe} \seq J}{
%       (\uctx, A^- ; \ofill{\octxe} \seq J \text{\ stable}) &
%       \uctx, A^- ; \ofill{\lfoc{A^-}} \seq J}
%     \\
%     \infer[\lab{lax}]{\uctx ; \octx \seq A^+ \lax}{
%       \uctx ; \octx \seq \rfoc{A^+}}
%     \and
%     \infer[\rlab{\monad{}}]{\uctx ; \octx \seq \monad{A^+}}{
%       \uctx ; \octx \seq A^+ \lax}
%     \and
%     \infer[\llab{\monad{}}]{\uctx ; \omatch{\lfoc{\monad{A^+}}} \seq C^+ \lax}{
%       \uctx ; \ofill{A^+} \seq C^+ \lax}
%   \end{mathpar}
%   \caption{A focused sequent calculus for ordered lax logic.\label{fig:focused-ordered-logic}}
% \end{figure}%

\subsubsection{}\label{sec:xyz}

%
\NewPredicate{\eps}[{\smash[b]{\mathsf{eps}}}]{0}%
\expandafter\NewPredicate\expandafter{\csname bit0\endcsname}[bit_0]{0}%
\expandafter\NewPredicate\expandafter{\csname bit1\endcsname}[bit_1]{0}%
\NewPredicate{\bitp}[bit]{0}%
\ExplSyntaxOn
\NewDocumentCommand{\bit}{O{} m O{}}{
  \tl_if_empty:nTF {#2}
    { \bitp[#1,#3] }
    { \use:c {bit#2} [#1,#3] }
}
\ExplSyntaxOff
\NewPredicate{\inc}{0}%
%

Viewed through a computational lens, proof search in a fragment of ordered logic becomes a forward-chaining logic programming language~\autocite{Pfenning+Simmons:LICS09}.
This form of ordered logic programming can be seen as a generalization of string rewriting~\autocite[see, \eg,][]{Book+Otto:SRS93}.

We can exploit this analogy to provide some intuition for forward-chaining ordered logic programming.
From the perspective of string rewriting, an ordered logic program's atomic propositions are letters; ordered conjunctions of these atoms are strings; and, under a focused proof search strategy~\autocite{Andreoli:JLC92}, the ordered implications that serve as program clauses are string rewriting rules.
An example will help to clarify.

\subsubsection{Example: Binary counter}\label{sec:exampl-binary-count-5}

Here we give an ordered logic program for an incrementable binary counter.
% We can implement an incrementable binary counter as an ordered logic program.
% From the perspective of string rewriting, atomic propositions are letters, and ordered conjunctions of these atoms are strings.
% 
% % In the string rewriting terminology, atomic propositions correspond to letters; ordered conjunctions of these atoms correspond to strings; and ordered implications correspond to string rewriting rules.
% In the string rewriting terminology, atomic propositions correspond to letters, and ordered conjunctions of these atoms correspond to strings.
% % Using a focused proof search strategy~\autocite{Andreoli:JLC92}, ordered implications correspond to string rewriting rules.
The counter is represented as a string of $\bit{0}$ and $\bit{1}$ atoms terminated at the most significant end by an $\eps$.
For instance, the\fxnote{\ \st{string, or}} ordered conjunction\fxnote{\st{,}} $\eps \fuse \bit{1} \fuse \bit{0}$ represents a counter with value $2$.
%
Increment instructions are represented by $\inc$ atoms at the counter's least significant end. 
% Also interspersed are $\inc$ atoms, each of which serves as an increment instruction sent to the counter given by the more significant bits.
Thus, $\eps \fuse \bit{1} \fuse \inc$ represents a counter with value $1$ that has been instructed to increment once.

Operationally, increments are described by the program's three clauses, the first of which is
\begin{equation*}
  \bit{1} \fuse \inc \lrimp \monad{\inc \fuse \bit{0}}
  \,.
\end{equation*}
This clause allows the string $\bit{1} \fuse \inc$ to be rewritten as $\inc \fuse \bit{0}$, in the sense that the rule
\begin{equation*}
  \infer{\uctx ; \bit{1} \fuse \inc \seq C \lax}{
    \uctx ; \inc \fuse \bit{0} \seq C \lax}
\end{equation*}
is admissible whenever this clause is part of the persistent context, $\uctx$.

Just as this implication transforms 
When this proposition is part of the persistent context $\uctx$, the rule
\begin{equation*}
  \infer{\uctx ; \omatch{\bit{1} \fuse \inc} \seq C \lax}{
    \uctx ; \ofill{\inc \fuse \bit{0}} \seq C \lax}
  \,.
\end{equation*}
is admissible.
Read bottom-up, this rule serves to rewrite the (sub)string $\bit{1} \fuse \inc$ as the string $\inc \fuse \bit{0}$.

Thus, by rewriting $\bit{1} \fuse \inc$ as $\inc \fuse \bit{0}$, this clause serves to carry the $\inc$ up past any $\bit{1}$s at the counter's least significant end.
Whenever the carried $\inc$ reaches the $\eps$ or right-most $\bit{0}$, the carry is resolved by one of the other two program clauses:
\begin{align*}
  &\eps \fuse \inc \lrimp \monad{\eps \fuse \bit{1}} \\
  &\bit{0} \fuse \inc \lrimp \monad{\bit{1}} \,.
\end{align*}
By rewriting $\eps \fuse \inc$ as $\eps \fuse \bit{1}$, the second clause ensures that the carry becomes a new most significant $\bit{1}$ in the $\eps$ case.
By rewriting $\bit{0} \fuse \inc$ as $\bit{1}$, the third clause ensures that the carry flips the $\bit{0}$ to $\bit{1}$ in that case.

For example, the counter $\eps \fuse \bit{1} \fuse \inc$ can be maximally rewritten as
\ExplSyntaxOn
\NewDocumentCommand{\mathul}{m}{
  \mathpalette\mathul:nn{#1}
}
\cs_new:Npn \mathul:nn #1#2 {
  \tikz [baseline] {
    \node (pr) [anchor = base, inner~sep = 0em] {$#1#2$};
    \draw [overlay, ultra~thick, gray]
      ([yshift=-0.3em]pr.base~west) -- ([yshift=-0.3em]pr.base~east);
  }
}
\ExplSyntaxOff
\begin{equation*}
  \eps \fuse \mathul{\bit{1} \fuse \inc}
    \trans \mathul{\eps \fuse \inc} \fuse \bit{0}
    \trans \eps \fuse \bit{1} \fuse \bit{0}
    \ntrans
  \text{\,,}
\end{equation*}
where the rewrite sites available at each step are underlined.
This trace computes $1 + 1 = 2$ in binary representation.
More generally, the above rewrite rules adequately specify an increment operation:
$C$ represents a counter with value $N$ if and only if $C \fuse \inc \trans+ C' \ntrans$ for some $C'$ that represents a counter of value $N + 1$.

\subsubsection{\emph{Concurrent} ordered logic programming}\label{sec:concurrency}

\NewPredicate{\coin}{0}
\NewPredicate{\heads}{0}
\NewPredicate{\tails}{0}
\begin{align*}
  &\coin \lrimp \monad{\heads} \\
  &\coin \lrimp \monad{\tails}
\end{align*}

\paragraph{Concurrency.}\label{sec:concurrency-1}

Some strings contain more than one rewrite site.
% several disjoint substrings that are amenable to rewriting.
% If these sites are disjoint, the rewrites can be thought of as happening concurrently.
For instance, the following binary counter has two $\inc$s in flight, which give rise to two disjoint rewrite sites.
\begin{equation*}
  \mathul{\eps \fuse \inc} \fuse \mathul{\bit{0} \fuse \inc}
\end{equation*}
The rewrites in this example can be interleaved in two ways: either
% there are two interleavings of these rewrites: either
\begin{align*}
  &\mathul{\eps \fuse \inc} \fuse \mathul{\bit{0} \fuse \inc}
     \trans \eps \fuse \bit{1} \fuse \mathul{\bit{0} \fuse \inc}
     \trans \eps \fuse \bit{1} \fuse \bit{1}
     \ntrans \\
  %
  \shortintertext{or}
  %
  &\mathul{\eps \fuse \inc} \fuse \mathul{\bit{0} \fuse \inc}
     \trans \mathul{\eps \fuse \inc} \fuse \bit{1}
     \trans \eps \fuse \bit{1} \fuse \bit{1}
     \ntrans
   \,.
\end{align*}
However, because these two rewrites are independent, they should be considered morally concurrent.
Rather than giving a truly concurrent semantics for string rewriting, we can treat different interleavings of independent steps as indistinguishable.
Then, because we can't observe which rewrite occurred first, the two rewrites appear to happen concurrently.
This is the idea of \vocab{concurrent equality} from \acs{CLF}~\autocite{Watkins+:CMU02}, adapted to string rewriting.


\paragraph{Infinite traces and fairness.}

\NewPredicate{\incs}{0}%
Thus far, all traces have been finite, but this is not necassarily so.
Consider adding an atom, $\incs$, that generates a stream of $\inc$ atoms:
\begin{equation*}
  \incs \lrimp \monad{\inc \fuse \incs}
  \,.
\end{equation*}
Among the infinite traces now possible is
% Beginning from $\eps \fuse \incs$, there are now infinite trace
\begin{equation*}
  \eps \fuse \mathul{\incs}
    \trans \mathul{\eps \fuse \inc} \fuse \mathul{\incs}
    \trans \dotsb
    \trans \mathul{\eps \fuse \inc} \fuse \inc \fuse \dotsb \fuse \inc \fuse \mathul{\incs}
    \trans \dotsb
  \,.
\end{equation*}

Notice that this trace never rewrites the infinitely available $\eps \fuse \inc$, instead always choosing to rewrite $\incs$.
Because of its\fxnote{\ \st{scheduling}} bias against rewriting $\eps \fuse \inc$, we say that the trace is \vocab{(weakly) transition-unfair}.

Transition unfairness is fundamentally at odds with true concurrency.
Because true concurrency allows multiple independent events to occur simultaneously, one event cannot preclude another independent event;
in the above example, for instance, rewriting the $\incs$ atom would not preclude rewriting the independent $\eps \fuse \inc$.
Therefore, to maintain the pretense of true concurrency, we must require all traces to be (weakly) transition-fair.


% Transition unfairness is fundamentally at odds with true concurrency
% because true concurrency allows multiple independent events to occur simultaneously;
% the occurrence of one event cannot preclude another independent event from occurring at the same time.
% To maintain the pretense of true concurrency, 

% In the above example, $\eps \fuse \inc$ and $\incs$ are independent rewrite sites

% In the above example, $\eps \fuse \inc$ and $\incs$ are independent rewrite sites.
% % ; if they are truly concurrent, the rewrite of $\eps \fuse \inc$ should eventually occur.
% To maintain any pretense of true concurrency, the rewrite of $\eps \fuse \inc$ should eventually occur since true concurrency would allow multiple independent events to occur simultaneously.



% However, by treating different interleavings of independent steps as indistinguishable, these two rewrites happen concurrently.




% For instance, notice that this binary counter specification allows multiple $\inc$s to be in flight at once, each of which is amenable to rewriting.



% Sometimes several disjoint substrings are amenable to rewriting.


% Different interleavings of independent steps are indistinguishable.

% String rewriting (and therefore ordered logic programming) gives rise to 


% The counter $\eps \fuse \inc \bit{0} \fuse \inc$ has two $\inc$s in flight, which can be rewritten independently.


% Rewrites of disjoint substrings can be thought of as happening concurrently.

% Notice that we can also allow multiple $\inc$s to be in flight at once, and that independent rewrites can thought of as happening concurrently.
% For instance, the counter $\eps \fuse \inc \fuse \bit{0} \fuse \inc$ has two $\inc$s in flight, and they give rise to independent rewrites.
% \begin{center}
%   \begin{tikzpicture}
%     \matrix [matrix of math nodes, column sep = 1.5em]
%     {
%       % First row
%       & |(inc-1-2)| \eps \fuse \bit{1} \fuse \mathul{\bit{0} \fuse \inc} & \\
%       % Second row
%       |(inc-2-1)| \mathul{\eps \fuse \inc} \fuse \mathul{\bit{0} \fuse \inc}
%         && |(inc-2-3)| \eps \fuse \bit{1} \fuse \bit{1} \\
%       % Third row
%       & |(inc-3-2)| \mathul{\eps \fuse \inc} \fuse \bit{1} & \\
%     };

%     \begin{scope}
%     [ start chain, every join/.style={->} ]
%       \chainin (inc-2-1);
%       \begin{scope}[start branch=inc-1-2]
%         \chainin (inc-1-2) [join];
%       \end{scope}
%       \begin{scope}[start branch=inc-3-2]
%         \chainin (inc-3-2) [join];
%       \end{scope}
%       \chainin (inc-2-3) [join = with inc-1-2, join = with inc-3-2];
%     \end{scope}
%   \end{tikzpicture}
% \end{center}


\subsubsection{Example: Binary counter with decrements}\label{sec:exampl-binary-count-3}

It's also possible to add support for decrements to the above ordered logic program.
%
\NewPredicate{\dec}{0}%
\NewPredicate{\zero}{0}%
\RenewPredicate{\succ}{0}%
%
Like increments, a decrement instruction is represented by a $\dec$ atom at the counter's least significant end.
To perform the decrement, a $\dec$ begins propogating up the counter.
As it passes over any $\bit{0}$s at the least significant end, they are marked as $\bit[']{0}$s to indicate that they are waiting to borrow from their more significant neighbors:
\begin{equation*}
  \bit{0} \fuse \dec \lrimp \monad{\dec \fuse \bit[']{0}} \,.
\end{equation*}
Whenever it reaches the $\eps$ or right-most $\bit{1}$, the $\dec$ is replaced with either $\zero$ or $\succ$, respectively, to show whether the borrow was possible; in the case of $\bit{1}$, the borrow is also effected:
\begin{align*}
  &\eps \fuse \dec \lrimp \monad{\eps \fuse \zero} \\
  &\bit{1} \fuse \dec \lrimp \monad{\bit{0} \fuse \succ} \,.
\end{align*}
Then the $\zero$ or $\succ$ travels back over all of the $\bit[']{0}$s that were waiting to borrow.
In the case of $\zero$, the bits are returned to their original $\bit{0}$ state because no borrow was possible;
in the case of $\succ$, a borrow was performed and so the bits are set to $\bit{1}$:
\begin{align*}
  &\zero \fuse \bit[']{0} \lrimp \monad{\bit{0} \fuse \zero} \\
  &\succ \fuse \bit[']{0} \lrimp \monad{\bit{1} \fuse \succ} \,.
\end{align*}


% According to the following rewrite rules, a $\dec$ propogates up the counter past any $\bit{0}$s until it reaches an $\eps$ or the right-most $\bit{1}$.
% At this point, the $\dec$ is replaced with either $\zero$ or $\succ$, respectively.
% \begin{align*}
%   &\bit{0} \fuse \dec \lrimp \monad{\dec \fuse \bit[']{0}} \\
%   &\eps \fuse \dec \lrimp \monad{\eps \fuse \zero} \\
%   &\bit{1} \fuse \dec \lrimp \monad{\bit{0} \fuse \succ}
% \end{align*}
% Then the $\zero$ or $\succ$ traveles back down the counter
% \begin{align*}
%   &\zero \fuse \bit[']{0} \lrimp \monad{\bit{0} \fuse \zero} \\
%   &\succ \fuse \bit[']{0} \lrimp \monad{\bit{1} \fuse \succ}
% \end{align*}


For example, the counter $\eps \fuse \bit{1} \fuse \bit{0} \fuse \dec$ can be maximally rewritten as
\begin{align*}
  \MoveEqLeft[0.5]
  \eps \fuse \bit{1} \fuse \mathul{\bit{0} \fuse \dec} \\
    &\trans \eps \fuse \mathul{\bit{1} \fuse \dec} \fuse \bit[']{0} \\
    &\trans \eps \fuse \mathul{\bit{0} \fuse \succ} \fuse \bit[']{0} \\
    &\trans \eps \fuse \bit{0} \fuse \bit{1} \fuse \succ \\
    &\ntrans
\end{align*}
Once again, there are possibilities for concurrency.
For example, the following two traces are indistinguishable because they differ only in the order of independent rewrites:
\begin{align*}
  &\mathul{\eps \fuse \inc} \fuse \mathul{\bit{0} \fuse \dec} \trans \mathul{\eps \fuse \inc} \fuse \dec \fuse \bit[']{0} \trans \eps \fuse \mathul{\bit{1} \fuse \dec} \fuse \bit[']{0} \\
  %
  \shortintertext{and}
  %
  &\mathul{\eps \fuse \inc} \fuse \mathul{\bit{0} \fuse \dec} \trans \eps \fuse \bit{1} \fuse \mathul{\bit{0} \fuse \dec} \trans \eps \fuse \mathul{\bit{1} \fuse \dec} \fuse \bit[']{0}
\end{align*}
This justifies treating those two rewrites as concurrent.


% c <- dec <- d =
% { case d of
%     eps => wait d;
%            d' <- eps;
%            c <- zero <- d'
%   | bit0 => d' <- dec <- d
%             c <- bit0' <- d'
%   | bit1 => d' <- bit0 <- d
%             c <- succ <- d' }

% c <- zero <- d =
% { case c of
%     bit0' => d' <- bit0 <- d
%              c <- zero <- d' }

% Cntr = +{ eps: 1 , bit0: Cntr , bit1: Cntr }
% Cntr' = &{ bit0': Cntr' }

% bit0 : {Cntr |- Cntr}
% dec : {Cntr |- Cntr'}
% zero : {Cntr |- Cntr'}
% bit0' : {Cntr' |- Cntr'}


\begin{align*}
  &\eps \fuse \dec \lrimp \monad{\eps \fuse \zero} \\
  &\bit{0} \fuse \dec \lrimp \monad[auto]{
                               \dec \fuse \parens[auto, align=c@{\,}l]{
                                                & (\zero \limp \monad{\bit{0} \fuse \zero}) \\
                                          \with & (\succ \limp \monad{\bit{1} \fuse \succ})}} \\
  &\bit{1} \fuse \dec \lrimp \monad{\bit{0} \fuse \succ}
\end{align*}






\paragraph{String rewriting rules as ordered implications.}

Using a focused proof search strategy~\autocite{Andreoli:JLC92}, ordered implications correspond to string rewriting rules.


\subsubsection{Example: Binary counter}\label{sec:exampl-binary-count-4}

As a running example, we can implement a binary counter that supports increments.
The counter is represented as a string of $\bit{0}$ and $\bit{1}$ letters terminated at the most significant end by an $\eps$.
So, for instance, the ordered conjunction, or string, $\eps \fuse \bit{1} \fuse \bit{0}$ represents a counter with value $2$.
Also interspersed are $\inc$ atoms, each of which serves as an increment instruction sent to the counter given by the more significant bits.
Thus, $\eps \fuse \bit{1} \fuse \inc$ represents a counter with value $1$ that has been sent an increment instruction.

Operationally, increments are described by three ordered implications that correspond to string rewriting rules; the first of these is
\begin{equation*}
  \bit{1} \fuse \inc \rimp \monad{\inc \fuse \bit{0}} \,.
\end{equation*}
By rewriting the string $\bit{1} \fuse \inc$ as $\inc \fuse \bit{0}$, this rule carries the $\inc$ up past any $\bit{1}$s at the counter's least significant end.
Whenever the carried $\inc$ reaches the $\eps$ or right-most $\bit{0}$, the carry is resolved:
\begin{align*}
  &\eps \fuse \inc \lrimp \monad{\eps \fuse \bit{1}} \\
  &\bit{0} \fuse \inc \lrimp \monad{\bit{1}} \,.
\end{align*}
By rewriting $\eps \fuse \inc$ as $\eps \fuse \bit{1}$, the second rule ensures that the carry becomes a new most significant $\bit{1}$ in the $\eps$ case.
By rewriting $\bit{0} \fuse \inc$ as $\bit{1}$, the third rule ensures that the carry flips the $\bit{0}$ to $\bit{1}$ in that case.



When this proposition is part of the persistent context $\uctx$, the following rule is derivable:
\begin{equation*}
  \infer{\uctx ; \omatch{\bit{1}, \inc} \seq J}{
    \uctx ; \ofill{\inc, \bit{0}} \seq J}
  \,.
\end{equation*}
Read bottom-up, this derived rule rewrites part of the ordered context so that $\bit{1}, \inc$ becomes $\inc, \bit{0}$.


\begin{equation*}
  \infer[\lab{copy}]{\uctx ; \omatch{\bit{1}, \inc} \seq J}{
    \infer[\llab{{\rimp}}]{\uctx ; \ofill{\bit{1}, \inc, (\bit{1} \fuse \inc \rimp \inc \fuse \bit{0})} \seq J}{
      \infer[\rlab{{\fuse}}]{\uctx ; \bit{1}, \inc \seq \bit{1} \fuse \inc}{
        \infer[\lab{id}]{\uctx ; \bit{1} \seq \bit{1}}{
          } &
        \infer[\lab{id}]{\uctx ; \inc \seq \inc}{
          }} &
      \infer[\llab{{\fuse}}]{\uctx ; \ofill{\inc \fuse \bit{0}} \seq J}{
        \uctx ; \ofill{\inc, \bit{0}} \seq J}}}
\end{equation*}




\subsubsection{Example: Binary counter}\label{sec:exampl-binary-count-2}

As an example of an ordered logic program, we can implement a binary counter that supports increments.
Similarly to the process implementation from \cref{sec:exampl-binary-count}, the counter is represented as a list of $\bit{0}$ and $\bit{1}$s terminated at the most significant end by an $\eps$.
Here, however, the $\bit{}$s and $\eps$ are not processes, but rather atomic propositions (or, in string rewriting terminology, letters).
For instance, the ordered conjunction (or string) $\eps \fuse \bit{1} \fuse \bit{0}$ represents a counter with value $2$.

% An increment instruction is represented by an $\inc$ atom at the counter's least significant end.
% There are three rewrite rules that describe the increment operation:
% \begin{align*}
%   &\eps \fuse \inc \lrimp \monad{\eps \fuse \bit{1}} \\
%   &\bit{0} \fuse \inc \lrimp \monad{\bit{1}} \\
%   &\bit{1} \fuse \inc \lrimp \monad{\inc \fuse \bit{0}}
% \end{align*}
% By rewriting $\eps \fuse \inc$ as $\eps \fuse \bit{1}$, the first rule introduces $\bit{1}$ as a new most significant bit, and thereby serves to increment an $\eps$.
% % By rewriting $\eps \fuse \inc$ as $\eps \fuse \bit{1}$ and thereby introducing $\bit{1}$ as a new most significant bit, the first rule serves to increment $\eps$s.
% Likewise, the second rule serves to increment a counter whose least significant bit is $\bit{0}$, by rewriting $\bit{0} \fuse \inc$ as $\bit{1}$ and thereby flipping $\bit{0}$.
% % Likewise, by rewriting $\bit{0} \fuse \inc$ as $\bit{1}$ and thereby flipping $\bit{0}$, the second rule serves to increment a counter whose least significant bit is $\bit{0}$.
% Finally, by rewriting $\bit{1} \fuse \inc$ as $\inc \fuse \bit{0}$, the third rule flips $\bit{1}$ and propogates a carry to the more significant bits, thereby serving to increment a counter whose least significant bit is $\bit{1}$.

An increment instruction is represented by an $\inc$ atom at the counter's least significant end.
There are three rewrite rules that describe increments, the first of which is
\begin{equation*}
  \bit{1} \fuse \inc \lrimp \monad{\inc \fuse \bit{0}} \,.
\end{equation*}
By rewriting $\bit{1} \fuse \inc$ as $\inc \fuse \bit{0}$, this rule carries the $\inc$ up past any $\bit{1}$s at the counter's least significant end.
Whenever the carried $\inc$ reaches the $\eps$ or right-most $\bit{0}$, the carry is resolved:
\begin{align*}
  &\eps \fuse \inc \lrimp \monad{\eps \fuse \bit{1}} \\
  &\bit{0} \fuse \inc \lrimp \monad{\bit{1}} \,.
\end{align*}
By rewriting $\eps \fuse \inc$ as $\eps \fuse \bit{1}$, the second rule ensures that the carry becomes a new most significant $\bit{1}$ in the $\eps$ case.
By rewriting $\bit{0} \fuse \inc$ as $\bit{1}$, the third rule ensures that the carry flips the $\bit{0}$ to $\bit{1}$ in that case.

% For example, after two rewrites, the counter $\eps \fuse \bit{1} \fuse \inc$ becomes $\eps \fuse \bit{1} \fuse \bit{0}$: using an abbreviated notation, the sequence of rewrites is 


% \begin{center}
%   \begin{tikzpicture}
%   [
%     rewrite source/.style = {fill = gray},
%     rewrite target/.style = {}
%   ]
%     \matrix [matrix of math nodes,
%              column sep = 0pt,
%              cells = {right}]
%     {
%       % First row:
%       && |(e1-1) [rewrite target]| \mathsf{e\,b_1} & |(0i-1) [rewrite source]| \mathsf{b_0\,i} && \\
%       % Second row:
%       |(ei-2) [rewrite source]| \mathsf{e\,i} & |(0i-2) [rewrite source]| \mathsf{b_0\,i}
%         &&& |(e1-2) [rewrite target]| \mathsf{e\,b_1} & |(1-2) [rewrite target]| \mathsf{b_1} \\
%       % Third row:
%       && |(ei-3) [rewrite source]| \mathsf{e\,i} & |(1-3) [rewrite target]| \mathsf{b_1} && \\      
% % % column sep = 1.5em, matrix of nodes, cells = {right}] {
% %                                   & |(e10i)| \fourbits e{b_1}{b_0}i & \\
% %       |(ei0i)| \fourbits ei{b_0}i &                                 & |(e11)| \threebits e{b_1}{b_1} \\
% %                                   & |(ei1)| \threebits ei{b_1}      & \\
%     };

%     % \begin{scope}
%     % [ start chain, every join/.style={->} ]
%     %   \chainin (ei0i);
%     %   \begin{scope}[start branch=e10i]
%     %     \chainin (e10i) [join];
%     %   \end{scope}
%     %   \begin{scope}[start branch=ei1]
%     %     \chainin (ei1) [join];
%     %   \end{scope}
%     %   \chainin (e11) [join = with e10i, join = with ei1];
%     % \end{scope}
%   \end{tikzpicture}
% \end{center}





\NewPredicate{\epsq}[eps{?}]{0}%
\NewPredicate{\yes}{0}%
\NewPredicate{\no}{0}%
\begin{align*}
  &\eps \fuse \dec \lrimp \monad{\eps} \\
  &\bit{0} \fuse \dec \lrimp \monad{\dec \fuse \bit{1}} \\
  &\bit{1} \fuse \dec \lrimp \monad[auto]{
                               \epsq \fuse \parens[auto, align=c@{\,}l]{
                                                 & (\yes \limp \monad{\one}) \\
                                           \with & (\no \limp \monad{\bit{0}})}}
  \\
  &\eps \fuse \epsq \lrimp \monad{\eps \fuse \yes} \\
  &\bit{0} \fuse \epsq \lrimp \monad{\bit{0} \fuse \no} \\
  &\bit{1} \fuse \epsq \lrimp \monad{\bit{1} \fuse \no}
\end{align*}
Or, push some details into the choreography:
\begin{align*}
  &\eps \fuse \dec \lrimp \monad{\eps} \\
  &\bit{0} \fuse \dec \lrimp \monad{\dec \fuse \bit{1}} \\
  &\eps \fuse \bit{1} \fuse \dec \lrimp \monad{\eps} \\
  &\bit{0} \fuse \bit{1} \fuse \dec \lrimp \monad{\bit{0} \fuse \bit{0}} \\
  &\bit{1} \fuse \bit{1} \fuse \dec \lrimp \monad{\bit{1} \fuse \bit{0}}
\end{align*}




\NewPredicate{\rend}[rend]{0}
\begin{align*}
  &\rend \fuse \inc \lrimp \monad{\inc \fuse \rend}     &&\rend \fuse \dec \lrimp \monad{\dec \fuse \rend[']} \\
  &\bit{1} \fuse \inc \lrimp \monad{\inc \fuse \bit{0}} &&\bit{0} \fuse \dec \lrimp \monad{\dec \fuse \bit[']{0}} \\
  &\eps \fuse \inc \lrimp \monad{\eps \fuse \bit{1}}    &&\eps \fuse \dec \lrimp \monad{\eps \fuse \zero} \\
  &\bit{0} \fuse \inc \lrimp \monad{\bit{1}}            &&\bit{1} \fuse \dec \lrimp \monad{\bit{0} \fuse \succ} \\
  &                                                     &&\zero \fuse \bit[']{0} \lrimp \monad{\bit{0} \fuse \zero} \\
  &                                                     &&\succ \fuse \bit[']{0} \lrimp \monad{\bit{1} \fuse \succ} \\
  &                                                     &&\zero \fuse \rend['] \lrimp \monad{\rend} \\
  &                                                     &&\succ \fuse \rend['] \lrimp \monad{\rend}
\end{align*}

\begin{sillcode*}
stype Nat = &{ inc: Nat , dec: Nat }
stype Counter = &{ inc: Counter , dec: Counter' }
  and Counter' = +{ zero: Counter , succ: Counter }

rend : unit -> {Counter |- Nat}
c <- rend () <- d =
{ case c of
    inc => ...
  | dec => d' <- dec () <- d;
           (case d' of
              zero => c <- rend () <- d'
            | succ => c <- rend () <- d') }

bit0 : unit -> {Counter |- Counter}
c <- bit0 () <- d =
{ case c of
    inc => ...
  | dec => d' <- dec () <- d;
           (case d' of
              zero => c' <- bit0 () <- d';
                      c <- zero () <- c'
            | succ => c' <- bit1 () <- d';
                      c <- succ () <- c') }

dec : unit -> {Counter |- Counter'}
dec : unit -> {Nat |- Nat}
c <- dec () <- d =
{ d.dec;
  c <- d }
\end{sillcode*}



\expandafter\NewPredicate\expandafter{\csname trit+\endcsname}[trit_{+}]{0}
\expandafter\NewPredicate\expandafter{\csname trit0\endcsname}[trit_{0}]{0}
\expandafter\NewPredicate\expandafter{\csname trit-\endcsname}[trit_{-}]{0}
\NewDocumentCommand{\trit}{O{} m}{\csname trit#2\endcsname[#1]}
\begin{align*}
  &\eps \fuse \inc \lrimp \monad{\eps \fuse \trit{+}} \\
  &\trit{+} \fuse \inc \lrimp \monad{\inc \fuse \trit{-}} \\
  &\trit{0} \fuse \inc \lrimp \monad{\trit{+}} \\
  &\trit{-} \fuse \inc \lrimp \monad{\trit{0}}
  %
  \\[\baselineskip]
  %
  &\eps \fuse \dec \lrimp \monad{\eps \fuse \trit{-}} \\
  &\trit{+} \fuse \dec \lrimp \monad{\trit{0}} \\
  &\trit{0} \fuse \dec \lrimp \monad{\trit{-}} \\
  &\trit{-} \fuse \dec \lrimp \monad{\dec \fuse \trit{+}}  
\end{align*}


\subsubsection{Technical details}\label{sec:technical-details}

\NewDocumentCommand{\sig}{}{\Sigma}

\begin{alignat*}{3}
  &\text{Negative Propositions} &\quad&& A^- &::= A^+ \rimp B^- \mid A^+ \limp B^- \mid A^- \with B^- \mid \monad{A^+} \\
  &\text{Positive Propositions} &&& A^+ &::= p^+ \mid A^+ \fuse B^+ \mid \one \mid A^-
\end{alignat*}


\NewDocumentCommand{\pseq}{}{\Vdash}
\NewDocumentCommand{\pof}{m}{#1}
\NewDocumentCommand{\pfuse}{m m}{#1 \fuse #2}
\NewDocumentCommand{\pone}{}{\one}

\NewDocumentCommand{\spof}{m}{#1}
\NewDocumentCommand{\spnil}{}{\mathsf{nil}}
\NewDocumentCommand{\sprapp}{m}{#1}
\NewDocumentCommand{\splapp}{m}{#1}
\ExplSyntaxOn
\NewDocumentCommand{\sppi}{t1 t2 m}{
  \IfBooleanTF {#1}
    { \IfBooleanTF {#2}
        { ERROR }
        { \sppi_i:n {#3} } }
    { \IfBooleanTF {#2}
        { \sppi_ii:n {#3} }
        { ERROR } }
}

\cs_new:Npn \sppi_i:n #1 { \pi\sb{1}; #1 }
\cs_new:Npn \sppi_ii:n #1 { \pi\sb{2}; #1 }
\ExplSyntaxOff

\begin{alignat*}{3}
  &\text{Patterns} &\quad&& p &::= x \mid \pfuse{p_1}{p_2} \mid \pone \\
  &\text{Substitutions} &&& \theta &::= \cdot \mid y/x, \theta \mid N/x, \theta \\
  &\text{Spines} &&& S &::= \sprapp{V ; S} \mid \splapp{V ; S} \mid \sppi1{S} \mid \sppi2{S} \mid \spnil \\
\end{alignat*}

\NewDocumentCommand{\lfoc}{m}{[#1]}
\NewDocumentCommand{\rfoc}{m}{[#1]}

\NewDocumentCommand{\tof}{m}{#1}
\NewDocumentCommand{\rlam}{m}{\lambda #1}
\NewDocumentCommand{\llam}{m}{\lambda #1}
\NewDocumentCommand{\pair}{m}{\langle #1 \rangle}
\ExplSyntaxOn
\NewDocumentCommand{\tlet}{>{\SplitArgument{1}{in}}m}
  { \tlet:nn #1 }
\cs_new:Npn \tlet:nn #1#2 { \{\mathsf{let}\;#1\;\mathsf{in}\;#2\} }
\ExplSyntaxOff

\NewDocumentCommand{\aof}{m}{#1}
\ExplSyntaxOn
\NewDocumentCommand{\atm}{>{\SplitArgument{1}{.}}m}
  { \atm:nn #1 }
\cs_new:Npn \atm:nn #1#2 { #1 \cdot #2 }
\ExplSyntaxOff

\NewDocumentCommand{\trof}{m}{#1}
\NewDocumentCommand{\tre}{}{\mathord{\diamond}}
\NewDocumentCommand{\trseq}{m}{#1}

\NewDocumentCommand{\stof}{m}{#1}
\ExplSyntaxOn
\NewDocumentCommand{\step}{>{\SplitArgument{1}{<-}}m}
  { \step:nn #1 }
\cs_new:Npn \step:nn #1#2 { #1 \leftarrow #2 }
\ExplSyntaxOff

\NewDocumentCommand{\vof}{m}{#1}


\ExplSyntaxOn
\NewDocumentCommand{\susp}{t+ t- m}{
  \IfBooleanTF {#1}
    { \IfBooleanTF {#2}
        { ERROR }
        { \susp_pos:n {#3} } }
    { \IfBooleanTF {#2}
        { \susp_neg:n {#3} }
        { ERROR } }
}

\cs_new:Npn \susp_pos:n #1 { \susp_neut:n {#1} }
\cs_new:Npn \susp_neg:n #1 { \susp_neut:n {#1} }
\cs_new:Npn \susp_neut:n #1 { \langle #1 \rangle }
\ExplSyntaxOff

\begin{mathpar}
  \infer{\omatch{x{:}A^-} \seq \aof{\atm{x . S} : \susp-{C^-}}}{
    \ofill{\lfoc{A^-}} \seq S : \susp-{C^-}}
  \and
  \infer{\omatch{\octxe} \seq \aof{\atm{c . S} : \susp-{C^-}}}{
    c{:}A^- \in \sig &
    \ofill{\lfoc{A^-}} \seq S : \susp-{C^-}}
\end{mathpar}

\begin{mathpar}
  \infer{\omatch{\lfoc{A^+ \rimp B^-}, \octx} \seq \spof{\sprapp{V;S} : \susp-{C^-}}}{
    \octx \seq \vof{V : \rfoc{A^+}} &
    \ofill{\lfoc{B^-}} \seq \spof{S : \susp-{C^-}}}
  \and
  \infer{\omatch{\octx, \lfoc{A^+ \limp B^-}} \seq \spof{\splapp{V;S} : \susp-{C^-}}}{
    \octx \seq \vof{V : \rfoc{A^+}} &
    \ofill{\lfoc{B^-}} \seq \spof{S : \susp-{C^-}}}
  \and
  \infer{\omatch{\lfoc{A^- \with B^-}} \seq \spof{\sppi1{S} : \susp-{C^-}}}{
    \ofill{\lfoc{A^-}} \seq \spof{S : \susp-{C^-}}}
  \and
  \infer{\omatch{\lfoc{A^- \with B^-}} \seq \spof{\sppi2{S} : \susp-{C^-}}}{
    \ofill{\lfoc{B^-}} \seq \spof{S : \susp-{C^-}}}
  \and
  \infer{\omatch{\lfoc{\monad{A^+}}} \seq \spof{\spnil : \susp-{\monad{A^+}}}}{
    }
\end{mathpar}

\begin{mathpar}
  \infer{x{:}\susp+{p^+} \pseq \pof{x : p^+}}{
    }
  \and
  \infer{\octx_1, \octx_2 \pseq \pof{\pfuse{p_1}{p_2} : A^+_1 \fuse A^+_2}}{
    \octx_1 \pseq \pof{p_1 : A^+_1} &
    \octx_2 \pseq \pof{p_2 : A^+_2}}
  \and
  \infer{\octxe \pseq \pof{\pone : \one}}{
    }
  \and
  \infer{x{:}A^- \pseq \pof{x : A^-}}{
    }
\end{mathpar}

\begin{mathpar}
  \infer{\octxe \seq \cdot : \octxe}{
    }
  \and
  \infer{y{:}\susp+{p^+}, \octx_0 \seq y/x, \theta : (x{:}\susp+{p^+}, \octx')}{
    \octx_0 \seq \theta : \octx'}
  \and
  \infer{\octx, \octx_0 \seq N/x, \theta : (x{:}A^-, \octx')}{
    \octx \seq N : A^- &
    \octx_0 \seq \theta : \octx'}
\end{mathpar}

\begin{mathpar}
  \infer{\octx \seq \vof{\sigma(p) : \rfoc{A^+}}}{
    \octx_A \pseq \pof{p : A^+} &
    \octx \seq \sigma : \octx_A}
\end{mathpar}

\begin{mathpar}
  \infer{\octx \seq \tof{\rlam{p.N} : A^+ \rimp B^-}}{
    \octx_A \pseq \pof{p : A^+} &
    \octx, \octx_A \seq \tof{N : B^-}}
  \and
  \infer{\octx \seq \tof{\llam{p.N} : A^+ \limp B^-}}{
    \octx_A \pseq \pof{p : A^+} &
    \octx_A, \octx \seq \tof{N : B^-}}
  \and
  \infer{\octx \seq \tof{\pair{N_1, N_2} : A^-_1 \with A^-_2}}{
    \octx \seq \tof{N_1 : A^-_1} &
    \octx \seq \tof{N_2 : A^-_2}}
  \and
  \infer{\octx \seq \tof{\tlet{T in V} : \monad{A^+}}}{
    \trof{T :: \octx \trans* \octx'} &
    \octx' \seq \vof{V : \rfoc{A^+}}}
\end{mathpar}

\begin{mathpar}
  \infer{\stof{\step{p <- R} :: \omatch{\octx} \trans \ofill{\octx_A}}}{
    \octx \seq \aof{R : \susp-{\monad{A^+}}} &
    \octx_A \pseq \pof{p : A^+}}
  \\
  \infer{\trof{\tre :: \octx \trans* \octx}}{
    }
  \and
  \infer{\trof{\trseq{T ; T'} :: \octx \trans* \octx''}}{
    \trof{T :: \octx \trans* \octx'} &
    \trof{T' :: \octx' \trans* \octx''}}
  \and
  \infer{\trof{S :: \octx \trans* \octx'}}{
    \trof{S :: \octx \trans \octx'}}
\end{mathpar}






\begin{itemize}
\item Suppose $c : p \rimp (\monad{p} \with \monad{\one})$.
Should $\bigl(\{x\} \leftarrow c \cdot (x; \pi_1)\bigr)^\omega :: x{:}p \trans[\omega]$ be considered a weakly fair execution?
I would argue that it should not be weakly fair because, with $c_1 : p \rimp \monad{p}$ and $c_2 : p \rimp \monad{\one}$, we certainly wouldn't say that $(\{x\} \leftarrow c_1 \cdot x)^\omega :: x{:}p \trans[\omega]$ is weakly fair.
%
\item By analogy, if $c : p \rimp (\monad{p} \with \monad{p})$, then $\bigl(\{x\} \leftarrow c \cdot (x; \pi_1)\bigr)^\omega :: x{:}p \trans[\omega]$ should not be considered weakly fair.
%
\item Notice that $p \rimp (\monad{p} \with \monad{p}) \dashv\vdash (p \llor p) \rimp \monad{p}$.
      Suppose $c : (p \llor p) \rimp \monad{p}$.
      Is $\bigl(\{x\} \leftarrow c \cdot (\mathsf{inl}\,x)\bigr)^\omega :: x{:}p \trans[\omega]$ a weakly fair execution?
      Following our reasoning for the corresponding $p \rimp (\monad{p} \with \monad{p})$, the answer would seem to be \enquote{no}.
%
\item However, now suppose $c : p \rimp \monad{p}$.
      I would argue that $(\{x\} \leftarrow c \cdot x)^\omega :: x{:}p, y{:}p \trans[\omega]$ \emph{is} a weakly fair execution, because the transition is made regardless of whether $x$ or $y$ is the justification.
%
\item To be consistent, it shouldn't matter whether $\mathsf{inl}\,x$ or $\mathsf{inr}\,x$ is the justification in the previous example.
      To resolve the contradiction, I would say that two transitions are distinct if they arise from distinct monadic heads.
\end{itemize}






\NewDocumentCommand{\hole}{}{\square}
\NewDocumentCommand{\pval}{o m}{%
  \IfValueT{#1}{#1 :} \rfoc{#2}}
\NewDocumentCommand{\vfuse}{m m}{#1 \fuse #2}
\NewDocumentCommand{\vone}{}{\one}
\NewDocumentCommand{\npat}{o m m}{%
  \IfValueT{#1}{#1 :} \lfoc{#2} \mathrel{>} #3}
\NewDocumentCommand{\pappr}{m m}{#1;#2}
\NewDocumentCommand{\pappl}{m m}{#1;#2}
\NewDocumentCommand{\pfst}{m}{\pi_1;#1}
\NewDocumentCommand{\psnd}{m}{\pi_2;#1}
\NewDocumentCommand{\pnil}{}{\text{\textsc{nil}}}
\NewDocumentCommand{\suspp}{m}{\susp+{#1}}
\NewDocumentCommand{\suspn}{m}{\susp-{#1}}

\NewDocumentCommand{\apair}{m m}{\langle #1, #2\rangle}

\begin{alignat*}{3}
  &\text{Normal Terms} &\quad&& N &::= \rlam{x}{N} \mid \llam{x}{N} \mid \apair{N_1}{N_2} \mid trace \\
  &\text{Atomic Terms}      &&& R &::= \atm{c}{S} \mid \atm{x}{S} \\
  &\text{Spines}            &&& S &::= \pnil \mid \pappr{V}{S} \mid \pappl{V}{S} \mid \pfst{S} \mid \psnd{S} \\
  &\text{Values}            &&& V &::= x \mid \vfuse{V_1}{V_2} \mid \vone \mid N
\end{alignat*}

\NewDocumentCommand{\erasespine}{m}{#1^e}
\NewDocumentCommand{\eraseatm}{m}{#1^e}
\begin{align*}
  &\erasespine{(\pnil)} = \pnil \\
  &\erasespine{(\pappr{V}{S})} = \pappr{\hole}{\erasespine{S}} \\
  &\erasespine{(\pappl{V}{S})} = \pappl{\hole}{\erasespine{S}} \\
  &\erasespine{(\pfst{S})} = \pfst{\erasespine{S}} \\
  &\erasespine{(\psnd{S})} = \psnd{\erasespine{S}}
  %
  \\
  %
  &\eraseatm{(\atm{c}{S})} = \atm{c}{\erasespine{S}} \\
  &\eraseatm{(\atm{x}{S})} = \atm{x}{\erasespine{S}}
\end{align*}


A maximal execution $\mathcal{T} :: \octx_0 \trans[\infty]_{\spec}$ is \emph{weakly unfair} if:
\begin{itemize}
\item there is an $i \geq 0$ such that $R :: \octx_j \trans_{\spec}$ and $\eraseatm{R} \neq \eraseatm{(R_j)}$ for all $j \geq i$.
\end{itemize}





\begin{mathpar}
  \infer{\octx_1, \octx_2 \seq \pval[\vfuse{V_1}{V_2}]{A^+ \fuse B^+}}{
    \octx_1 \seq \pval[V_1]{A^+} &
    \octx_2 \seq \pval[V_2]{B^+}}
  \and
  \infer{\octxe \seq \pval[\vone]{\one}}{
    }
  \and
  \infer{x{:}\suspp{p^+} \seq \pval[x]{p^+}}{
    }
  \and
  \infer{\octx \seq \pval[N]{A^-}}{
    \octx \seq N : A^-}
\end{mathpar}

\begin{mathpar}
  \infer{\ofill{\hole, x{:}\suspp{A^+}} \pseq \npat[\pappr{x}{S}]{A^+ \rimp B^-}{\gamma}}{
    \omatch{\hole} \pseq \npat[S]{B^-}{\gamma}}
  \and
  \infer{\ofill{x{:}\suspp{A^+}, \hole} \pseq \npat[\pappl{x}{S}]{A^+ \limp B^-}{\gamma}}{
    \omatch{\hole} \pseq \npat[S]{B^-}{\gamma}}
  \and
  \infer{\omatch{\hole} \pseq \npat[\pfst{S}]{A^- \with B^-}{\gamma}}{
    \ofill{\hole} \pseq \npat[S]{A^-}{\gamma}}
  \and
  \infer{\omatch{\hole} \pseq \npat[\psnd{S}]{A^- \with B^-}{\gamma}}{
    \ofill{\hole} \pseq \npat[S]{B^-}{\gamma}}
  \and
  \infer{\hole \pseq \npat[\pnil]{A^-}{\suspn{A^-}}}{
    }
\end{mathpar}

\begin{mathpar}
  \infer{\octx', \octx'_0 \seq \theta, V/x : \octx, x{:}\suspp{A^+}}{
    \octx'_0 \seq \pval[V]{A^+} &
    \octx' \seq \theta : \octx}
\end{mathpar}

\subsubsection{Generative invariants}\label{sec:gener-invar}

\NewPredicate{\cntr}{0}%
\begin{align*}
  &\cntr \lrimp \monad{\eps} \\
  &\cntr \lrimp \monad{\cntr \fuse \bit{0}} \\
  &\cntr \lrimp \monad{\cntr \fuse \bit{1}} \\
  &\cntr \lrimp \monad{\cntr \fuse \inc}
\end{align*}


% cntrq -> eps
% cntrq -> cntrq * bit0
% cntrq -> cntrq * bit1

% cntri -> eps
% cntri -> cntri * bit0
% cntri -> cntri * bit1
% cntri -> cntri * inc

% cntr' -> cntri * dec
% cntr' -> cntrq * zero
% cntr' -> cntrq * succ
% cntr' -> cntr' * (& ...)
% 
% with a valid state being either cntri or cntr'.


%%% Local Variables:
%%% TeX-master: "proposal"
%%% End:
