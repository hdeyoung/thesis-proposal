The key tenet of intuitionism
% , and, more generally, constructivism, 
is that mathematics is fundamentally an activity of construction.
To establish the existence of some mathematical object, one must construct the object; otherwise how can one be sure that it exists?
% show how to construct that object.
A proof of the truth of some proposition~$A$ is thus a construction of\fxnote{\ an object of \wc{type}}~$A$.
% describes a \wc{procedure}[blueprint] for constructing an object of \wc{type} $A$.
\fxnote{Proofs as constructions themselves, or proofs as procedures for construction?\ }%
% Stated more succinctly, proofs are constructions.
\fxnote{Brouwer's mental constructions vs.\@ formal proof objects?\ }%
% For Brouwer, these constructions exist only within the mind of the mathematician
For instance, in the \acf{BHK} interpretation of the intuitionistic logical connectives, a proof of the implication~$A \imp B$ is a construction that is capable of transforming proofs of~$A$ into proofs of~$B$---that is, a function.

% This \ac{BHK} interpretation describes an ideal function from proofs of $A$ to proofs of $B$.
% The terms `constuction' and `transforming' both have strong operational connotations.
Since computability theory gives a clear connection between functions and computation,
% It's therefore
it's natural to ask if there is also a connection between \emph{proofs} and computation\fxnote{\ from the intuitionistic proofs-as-constructions viewpoint}.
Two answers (at least) are possible: \vocab{proof-reduction-as-computation}\fxnote{, which is the basis for functional programming,} and \vocab{proof-search-as-computation}\fxnote{, which is the basis for logic programming}.

% Taking this proofs-as-constructions viewpoint, it is natural to ask if there is a connection between \wc{proofs}[constructions] and computation.
% Two proof-relevant notions of computation are possible: \vocab{proof-reduction-as-computation} and \vocab{proof-search-as-computation}.

\paragraph*{Proof reduction as computation.}
A typical exercise in polynomial functions from elementary algebra is \enquote{Given $f(x) = x^2 + x - 1$, find the value of $f(2)$.}
Just as a student repeatedly simplifies the expression~$f(2)$ to compute the value~$5$, so can we repeatedly \vocab{reduce} a proof of~$A$ to compute a simplest, most direct construction of~$A$---proof reduction as computation.  

Proof reductions were first described by \textcites{Gentzen:MZ35}{Prawitz:65}.
As an example of a reduction, suppose that by using the rule of \latin{modus ponens} we have a proof of~$A$ from proofs of~$B \imp A$ and~$B$.
% the rule of \latin{modus ponens} allows one to prove $B$ by proving both the implication $A \imp B$ and its antecedent, $A$.
According to the \ac{BHK} interpretation, our proof of~$B \imp A$ is a \wc{transformation}[function] from proofs of~$B$ to proofs of~$A$; therefore,
%, a proof reduction is possible:
% we can reduce our proof of $B$:
we just apply the transformation to our proof of~$B$ and thereby eliminate the \latin{modus ponens} detour and obtain a simpler, more direct proof of~$A$.
By continuing to reduce the resulting proof as much as possible, we can compute a simplest construction of~$A$.

When applied to formal proof objects, this notion of proof-reduction-as-computation is the basis for functional programming and is known as the Curry-Howard isomorphism~\autocites{Howard:Curry80}{Martin-Lof:LMPS80}:
A formal proof of proposition~$A$ is a program for computing a result value of type~$A$, and proof reductions correspond to steps in the program's evaluation---more succinctly, proofs are programs and propositions are types.

\begin{listing}[!t]
  \begin{pyglist}[language=sml, gobble=4, texcl=true]
    datatype nat = Z | S of nat
  
    (* plus : $\text{nat} \imp (\text{nat} \imp \text{nat})$ *) 
    fun plus Z n = n
      | plus (S m') n = S (plus m' n)
  \end{pyglist}
  \caption{\ac{SML} implementation of addition for unary natural numbers.\label{lst:smlplus}}
\end{listing}

Because they are proofs at heart, well-typed functional programs offer several guarantees about their behavior.
Most important is type safety, which is broken down into the following.
\begin{description}[font=\normalfont\itshape]
\item[Type preservation.]
  After each step of evaluation a program of type~$A$ continues to have type~$A$, just as a proof of~$A$ remains a proof of~$A$ after each reduction.
%
\item[Progress.]
  A well-typed program is either a value itself or it can make another step toward computing a value; in other words, \enquote{Well-typed programs don't get stuck.}
  Analogously, a proof is either as direct as possible, having no detours, or it can be reduced.
%
% \item[Termination.]
%   For inductive proofs, the progress property implies termination: reduction eventualy yields a simplest, most direct proof.
%   Similarly, 
\end{description}

To illustrate these concepts, a well-typed functional program for addition of unary natural numbers is shown in \cref{lst:smlplus}.
The program \sml`plus` is a function that takes two natural numbers and computes another natural number, their sum; accordingly, \sml`plus` has type $\text{\sml`nat`} \imp \text{\sml`nat`} \imp \text{\sml`nat`}$.
To compute the sum $1 + 1$, we evaluate \sml`plus (S Z) (S Z)`, which has type \sml`nat`.
After each step of evaluation, the residual program still computes a \sml`nat` (type preservation).
Moreover, because \sml`plus (S Z) (S Z)` is well-typed, evaluation will continue until the sum is computed (progress).
% In this case, because \sml`plus` is inductively defined, we also know that evaluation of \sml`plus m n` will \emph{eventually} compute the sum (termination).

% \paragraph*{}
% Proofs are not always given in the simplest, most direct way.
% \vocab{Proof reductions}, which were first described by \textcites{Prawitz:65}{Gentzen:MZ35} \fxnote[status=draft]{for formal proofs in the natural deduction and sequent calculi}, allow these proofs to be simplified.
% Then, just as we repeatedly simplify the numeric formula $(1 + 1) \times (2 + 3)$ to compute the value $10$, we can repeatedly reduce a proof of $A$ to compute a simplest construction of $A$---that is, proof reduction as computation.

% For example, suppose that by using the rule of \latin{modus ponens} we have a proof of $B$ from proofs of the implication $A \imp B$ and its antecedent, $A$.
% % the rule of \latin{modus ponens} allows one to prove $B$ by proving both the implication $A \imp B$ and its antecedent, $A$.
% Since a proof of $A \imp B$ is a \wc{transformation}[function] from proofs of $A$ to proofs of $B$, a proof reduction is possible:
% % we can reduce our proof of $B$:
% We just apply the transformation to our proof of $A$ to obtain a simpler, more direct proof of $B$.

% Computation is the act of repeatedly applying proof reductions until the proof
% % reducing a proof until it 
% is in its simplest, most direct form.

\paragraph*{Proof search as computation.}
Rather than assigning computational meaning to the proofs-as-constructions themselves, one could instead
% view the mathematician's \emph{search} for a construction as a computational process.
emphasize the mathematician's \emph{search} for a construction.
This is the idea underlying logic programming~\autocites{Colmerauer+:73}{Kowalski:IFIP74}---%
computation is the act of searching, according to a fixed strategy, for a proof of a \vocab{goal formula}, or query.
Differently than functional programming, a logic program is not a proof but a collection of \wc{axioms}[logical hypotheses] (\ie, propositions) under which the search occurs.

% Having a fixed search strategy allows the programmer to reason about the operational semantics of his programs.
Having a \emph{fixed} search strategy is what allows proof search to be used as a programming language; without a fixed strategy, there would be far too much nondeterminism in search for the programmer to reason about the operational semantics of his programs.
% 
One common strategy is \vocab{goal-directed search}: it consists of
% the search proceeds by
looking for an axiom that could be the final step in a proof of the goal and then
% continues recursively on the premises of these axioms.
continuing recursively to search for proofs of the premises of this axiom.
(This strategy is also known as backward-chaining or top-down search.)

\NewPredicate{\nat}{0}
\NewPredicate{\zero}[z]{0}
\RenewPredicate{\succ}[s]{1}
\NewPredicate{\prop}{0}
\NewPredicate{\plus}{3}
\begin{figure}[!tbp]
  \begin{align*}
    &\begin{alignedat}{2}
       &\zero &&: \nat \:. \\
       &\succ{} &&: \nat \to \nat \:.
     \end{alignedat}
    \\
    &\begin{alignedat}{2}
       &\plus{} &&: \nat \to \nat \to \nat \to \prop \:. \\
       &\rulename{plus-z} &&: \plus{\zero,N,N} \:. \\
       &\rulename{plus-s} &&: \!\begin{aligned}[t]
                                  \MoveEqLeft[0.5]
                                  \plus{(\succ{M'}),N,(\succ{O'})} \\[-\jot]
                                    &\pmi \plus{M',N,O'} \:.
                                \end{aligned}
     \end{alignedat}
  \end{align*}
  \caption{The addition relation on unary natural numbers given as a \wc{goal-directed}[backward-chaining] logic program.  By convention, uppercase variables are universally quantified and $A \pmi B$ is notation for the implication $B \imp A$.\label{fig:plus-lp}}
\end{figure}
For example, the addition relation on unary natural numbers is defined by the two clauses shown in \cref{fig:plus-lp}; that is, $m + n = o$ if and only if $\plus{m,n,o}$ is provable.
To compute the sum $1 + 1$, we
% use a goal-directed strategy to 
will search for the natural number~$O$ for which there is a proof of the goal $\plus{(\succ{\zero}),(\succ{\zero}),O}$.
% Because this is a goal-directed proof search, we begin by looking for an axiom that could be the last step in a proof of the goal.
We first notice that the $\rulename{plus-s}$ clause could be the final step in such a proof, so long as there is a proof of the premise $\plus{\zero,(\succ{\zero}),O'}$ with $O = \succ{O'}$.
Then, we can derive this subgoal, with $O' = \succ{\zero}$, by using the $\rulename{plus-z}$ clause.
Putting these together, our search succeeds with a proof of the original goal, $\plus{(\succ{\zero}),(\succ{\zero}),O}$, with $O = \succ{(\succ{\zero})}$: that is, $1 + 1 = 2$.
From this point of view, $\plus{}$ specifies a function: it takes its first two arguments as inputs and produces the third argument as a unique output.
In logic programming terminology, $\plus{}$ is said to have mode $(+,+,-)$.%
% is a ternary predicate with mode $(+,+,-)$
\fxnote{\ Notation for \emph{unique} output?}

Of course, not all goals are provable and so proof search must sometimes fail.
Unlike well-typed functional programs, logic programs therefore can't guarantee that a computation will always be able to make progress.
For example, just as there is no natural number solution to $1 + N = 0$, any search for a proof of $\plus{(\succ{\zero}),N,\zero}$ will get stuck.
% on natural numbers, the difference $0 - 1$ is undefined.
In this way, logic programming is strictly weaker than functional programming in terms of the behavioral guarantees that it provides.

% It's also possible to view $\plus{}$ as specifying the partial function for subtraction of natural numbers.
% Searching for the natural number $N$ for which $\plus{m,N,o}$ 


% Unlike \wc{proof-reduction-as-computation}[functional programming] in which the existence of a construction is guaranteed by programmer provides the construction, logic programming does not guarantee existence of a construction, proof search may fail to find a construction.

% \begin{table}
%   \begin{tabular}{@{}ll@{}} \toprule
%     \multicolumn{2}{c}{mode of computation} \\ \cmidrule{1-2}
%     proof reduction & proof search \\ \midrule
%     functional programming & logic programming \\
%     propositions as types & propositions as programs \\
%     proofs as programs & {?} \\ \bottomrule
%   \end{tabular}
% \end{table}

\paragraph{From logic programs to functional programs.}
Some algorithms are more naturally and concisely expressed as logic programs than as functional programs.
\fxnote{No example?\ }%
However, if behavioral guarantees are required, the programmer is generally forced to give up on the logic program and write a functional program instead.

Generally, but not always.
If the logic program is deterministic and defines a function rather than a relation, one can imagine compiling it to a well-typed functional program.
\fxnote{No references yet.\ }%
This way, the resulting functional program shares its behavioral guarantees with the source logic program, affording the programmer the best of both worlds.
Compilation would also make the program more efficient by eliminating the interpretive overhead of the logic programming engine.
As an example, one could imagine compiling the $\plus{}$ logic program from~\cref{fig:plus-lp} into the \sml`plus` function of~\cref{lst:smlplus} (or perhaps the A-normal form~\autocite{Sabry+Felleisen:LASC93} of \sml`plus`).


\textcite{Debray+Warren:TOPLAS89}
\textcites{Felleisen:IU86}{Haynes:JLP86} have identified classes of logic programs that may be compiled to well-typed functional programs.


\paragraph*{Linear Logic.}
The logic and functional programming languages that we've discussed thus far have been based on \citeauthor{Gentzen:MZ35}-style intuitionistic first-order logic.
Much research has also considered richer logics on which to base logic and functional programming languages.

One such logic is \citeauthor{Girard:TCS87}'s linear logic~\autocite*{Girard:TCS87}, which is suitable for modeling stateful systems.
Unlike in first-order logic, assumptions in linear logic must be used exactly once.
By using a linear assumption and then replacing it with a new assumption, 


\paragraph*{Linear Logic Programming.}


\paragraph*{Concurrent Functional Programming.}


\paragraph*{From Linear Logic Programs to Concurrent Functional Programs?}


\clearpage






%%% Local Variables:
%%% TeX-master: "proposal"
%%% End:
